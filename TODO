DONE

	Also watch "Latency Saves (% Daily)" we will see increases here as we fix more players.

	These is a signficant increase as I bumped packet loss sensitivity up to 0.1%. 

	I think this is because small amounts of packet loss tend to indicate a connection that might need help.

	Let SLOCLAP know about short sessions.

	Work up additional queries around short sessions

	Seems to be 8-9%, fairly uniform across regions, datacenters.

	There has been a significant reduction in vetos

	Look at "Debug Latency Not Fixed (% Daily)" to see if there is improvement.

	Significant reduction over the past few days

	Watch "Lost Route (% Daily)" to see if I can spot some improvement post fixes.

	Yes, there is a significant reduction.

	Write query to calculate max sessions per-relay

TODO

	Create a tool to print out relay ids

	Maybe just on "next database" then grep

	Or a new tool, "next relay_ids"

	------------
	
	To save money I should seriously consider downsizing some google cloud and AWS relays.

	At the session counts they have, many do not need more than 2 cores.

	This could save a lot of money!

	------------

	Vast majority of "Brasil Sessions Above 100ms Not Fixed" are because "all_client_relays_are_zero"

	This is the thing to fix now.

	no client relays now 100% tracks all client relays are zero

	so this is all the same problem. seems like I should try to fix this today.

	------------

	Look close over the next few days to see if recent changes have caused jitter to go up on next.

	------------

	Extend module to have nice helper functions for setting game latency, jitter and PL (static)

	------------

	*** Biggest win by far is fixing fallback to direct!!! ***

	Fallback to direct % does seem to be related to how far away from google cloud server backend.

		i3d.dubai is top (3%+)!!!
		google.frankfurt.1 is bottom (zero)

	But... OVH ashburn has 1.0%. This is pretty bad, it's not just distance, but how well connected servers are to google cloud?

	------------

	Add a server_not_accelerated flag in the session summary. Makes it easy to exclude these.

	------------

	Add "lag_spike" flag in summary. Any slice with direct rtt above 500ms.

	This should be broken up into:

		next_lag_spike
		direct_lag_spike

	------------

	next_jitter_spike
	direct_jitter_spike
	real_jitter_spike

	------------

	next_packet_loss_spike
	direct_packet_loss_spike
	real_packet_loss_spike

	------------

	It seems that duration_on_next is sometimes zero, even though a session has spent time on network next.

	What's going on here?!

	Example session:

		-8003309578772432229

	Add some analytics to track this case, and see how frequently it occurs. I think it's a pretty rare bug, but I'd like to be able to rely 100% on duration_on_next in queries

	-----

	Wait for the patch to go out. We should see a reduction in fallback to direct (longer timeouts).

	Plus, we will see the reasons for fallback to direct. Identify what is happening and think about how it could happen.

	Can we reduce it?

	-----

	Try to identify why abort is triggering.

	-----

	What is causing the remaining lost routes to happen?

	Dig in on some sessions to see what's causing it...

	It seems to have a strong correlation to datacenter_id

	Highest route lost %:

		latitude.saopaulo      ->  0.96%
		datapacket.istanbul    ->  0.66%
		i3d.dubai              ->  0.53%
		gcore.ashburn          ->  0.51%
		hivelocity.losangeles  ->  0.49%
		uk2group.dallas        ->  0.43%

		...

	Lowest route lost %:

		google.* are all at 0.0

		uk2group.frankfurt     ->  0.14%
		datapacket.frankfurt   ->  0.28%

		etc...

	What's causing this variation?

	-----

	Fallback to direct is counter productive because it remove visibility past the point of fallback.

	Instead, we can rely on veto and abort to say, OK, we shouldn't accelerate this person anymore

	But it's good for them to keep talking to the backend as long as the session exists, so we retain visibility!

	-----



















	-----

	Would be nice if I could get the session counts over to the left a bit so I don't need to include scrollbar in screenshots...

	-----

	Add a functional test to verify that we see the correct lat/longs passed up from the SDK

	-----

	Add a functional test to verify that we see server delta time min/max/avg passed up from SDK

	-----

	Add a functional test to verify we get game rtt, jitter and pl

	-----

	Add a functional test to verify we see flags

	-----

	Add functional test to verify we see match id from sessions

	-----

	Add functional test to verify we get the likely_vpn_or_cross_region

	-----

	If we end up doing source relay filtering for jitter, add some unit tests and func tests around this.

	-----

	Update documentation

	-----

	Fix some easy issues

	-----

	Make release

	-----





	-----

	Next release:

	-----------

	There's a lot of stuff that sets "state.Error" kinda assuming its persistent, but it's not.

	I'd feel more comfortable migrating this over to bools instead of flags.

	And the bools must be persisted in route state and then set in the session summary.

	Can't make this change live with REMATCH because it will remove some visibility of things going back before this change.

	-----------
