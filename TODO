DONE

	Write code to parse slice data

	Generate random slice data

	Integrate slice data into redis cruncher

	Generate random session data

	Integrate random session data into the redis cruncher

	Parse session data
	
	Unit test all portal data types, eg. generate random data, write to string, parse string and verify its the same.

TODO

	Add tests for serializing route matrix and cost matrix at maximum possible sizes

	-----------------

	Fix func tests
	
	-----------------

	Actually pull top session data, slice data and near relay data and return as structs

	-----------------

	Parameterize "get top sessions" to take start index and finish index

	This way we can paginate the session display, if we want to.

 	-----------------

	Write relay updates on a separate thread. "r-"

	-----------------

	Write server updates on a separate thread. "sv-"

	---------------------------------------------------------

	Generalize to multiple buyers

	---------------------------------------------------------










































































































































	--------------

	Get pubsub emulator setup.

	Now actually write the analytics messages to pubsub in happy path (local).

	Verify that the analytics service receives and processes the analytics messages.

	Setup the bigquery emulator.

	Actually write the analytics messages to bigquery...

	--------

	Emit the analytics database update message from relay_backend leader.

	--------

	Add server sdk to link to match

	next_server_match ( match_id )

	Make sure the match id gets passed up in both the server init, server update and the session updates from SDK -> backend.

	--------

	Add uint64 type to match data function

	Extend SDK to support multiple calls to match data

	Extend SDK to store an array of match data, and flush that array

	Extend match data packets to server backend to include type

	--------

	Match events ()

	next_server_match_event

	--------

	Session events (address)

	next_server_session_event

	--------

	Session data (address, type, array of doubles)

	next_server_session_data

	--------

	Get the pubsub emulator back up locally

	Get the process for setting up pubsub queues etc going via "run"

	eg. "run pubsub-create", "run pubsub-destroy" or whatever works

	--------

	Get the bigquery emulator up locally

	Decide on schemas for each of the table types, and put them under "bigquery"

	eg. "run bigquery-create", "run bigquery-destroy" or whatever.

	--------

	Extend analytics service to insert messages into bigquery.

	--------------

	Implement simple "api" service and get OAuth and 

	--------------









































Essentials:

	--------------

	Setup google cloud environment with terraform scripts

	--------------

	The relay should talk to the relay backend over SSH in dev and production.

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Multithreaded relay

	--------------

	Implement a fix for a re-ordering of route tokens / continue tokens to create loops. Order of tokens must be enforced.

	This probably means we need to have some sort of signed bit, that indicates that indeed, this is token n, n+1 etc.

	And then this can be checked.

	Needs to be designed...

	--------------

	The SDK must never transmit the user hash in plaintext. It must always be encrypted.

	--------------

	Get "pro" tagging back in. This is a cool feature.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Multipath across two network next routes.

	--------------





Nice to have:

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	--------------

	Extend the SDK5 so we have the option of sending down new near relay stats on multiple slices, later on.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	We are very close to removing libsodium from golang. It should be possible. If we do this, it becomes much easier to build and run golang services, as they are fully native.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	--------------

	Functional tests to verify that relay gateway, relay backend, server backend 5 are resilient to invalid or won't read in bad database.bin files

	--------------

	There should be relay functional tests.

	---------------

	Extend "next ssh" to ssh into google Vms by name, if no relay is found

	---------------

	I really can't answer the question "why are half the relays not carrying sessions in dev?"

	Is it just a property of the optimizer, eg. it will go away if I disable mispredict and set all costs to zero?

	Or is there some sort of bias causing this. It seems important to explore this and work it out.

	If there is some systemic bias that just makes some relays carry a lot of sessions while other relays don't... that's a serious problem in production.

	---------------

	We need a way to track no packets on client for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	We need a way to track no packets on server from session for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	---------------

	We absolutely need to track client and server frame hitches

	This would have helped Velan and PFG.

	---------------

	Leader election needs to wait at least 11 seconds to make sure it gets the correct result when > 2 vms start at the same time without flap

	---------------

	Leader election func tests for relay backends + ready delay

	---------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	---------------

	Create a new minimal "api" service that provides the JSONRPC calls necessary for the next tool to work.

	---------------

	Extend "next relays" to show "PublicAddress" and "InternalAddress" for relays.

	---------------

	Relay gateway verify keypair on startup

	Server backend verify keypairs on startup

	---------------

	Should be func tests that spin up the bigquery emulator and verify that each analytics message type can be written to the appropriate table created from schema

	---------------

	Plumb internal events all the way up from client SDK to backend.

	Client SDK must be able to trigger certain events (eg. client hitches, no packets received etc...)

	Server can trigger others, then they get OR'd before sending up to backend.

	--------

	Add internal session events. Track server hitches, client hitches, no packets received, over bandwidth etc.

	These should be passed on to both the portal for immediate visualization, and to bigquery for analysis.

	--------

	Redo pro, but do it via internal session events.

	--------
