DONE

	Get all func tests passing.

TODO
	
	Help Velan debug packet spikes and delayed packet receives.

	---------------

	Clean up and optimize protocol between relay and relay backend.

	We now have the opportunity to do this now without disruption.

	---------------

	Cost can be optimized now to use [0,255] uint8 values

	---------------

	Implement relay validation on join

		1. Verify the relay has the correct public and internal addresses

		2. Verify its public/private keypairs are valid (can encrypt/decrypt data)

 		3. Verify that it can read/write test tokens sent to it from the backend

		4. On the relay backend side, verify that it can sign data with the expected private key.

	---------------

	Implement new relay "internal group" concept.

	A string that is hashed. Only relays with the same internal group hash use internal addresses between each other.

	For example, for AWS relays in each region would have the same internal group string.

	Default should be "" which turns into 0

	---------------

	Implement unit tests for relay internal groups addresses.

	---------------







































	---------------

	Create a new minimal "api" service that provides the JSONRPC calls necessary for the next tool to work.

	--------------

	Extend "next relays" to show "PublicAddress" and "InternalAddress" for relays.

    --------------

    Add relay counters to relay stats message.

    --------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	--------------

	Leader election needs to wait at least 11 seconds to make sure it gets the correct result when > 2 vms start at the same time without flap

	--------------

	Leader election func tests for relay backends + ready delay

	--------------












































Essentials:

	--------------

    Send portal messages to redis streams

	--------------

	Portal cruncher

	--------------

	Send rest of server backend messages to google pubsub

	--------------

	Extend analytics service to insert messages into bigquery.

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Extend and clean up the current reference relay implementation so it is multithreaded.

	--------------

	Implement a fix for a re-ordering of route tokens / continue tokens to create loops. Order of tokens must be enforced.

	This probably means we need to have some sort of signed bit, that indicates that indeed, this is token n, n+1 etc.

	And then this can be checked.

	Needs to be designed...

	--------------

	Extend server events into, match events, match data, session events, session data

	--------------

	The SDK must never transmit the user hash in plaintext. It must always be encrypted.

	--------------

	Put "MatchId" into session data (first).

	This way we can easily look up from a session the match it is in.

	--------------

	Get "pro" tagging back in. This is a cool feature.

	--------------

	Extend pro tagging so the whole server gets tagged pro?

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------




Nice to have:

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	--------------

	Extend the SDK5 so we have the option of sending down new near relay stats on multiple slices, later on.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Multipath across two network next routes.

	--------------

	We are very close to removing libsodium from golang. It should be possible. If we do this, it becomes much easier to build and run golang services, as they are fully native.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	--------------

	Functional tests to verify that relay gateway, relay backend, server backend 5 are resilient to invalid or won't read in database.bin files being injected.

	--------------

	There should be relay functional tests.

	---------------

	Extend "next ssh" to ssh into google Vms by name, if no relay is found

	---------------

	I really can't answer the question "why are half the relays not carrying sessions in dev?"

	Is it just a property of the optimizer, eg. it will go away if I disable mispredict and set all costs to zero?

	Or is there some sort of bias causing this. It seems important to explore this and work it out.

	If there is some systemic bias that just makes some relays carry a lot of sessions while other relays don't... that's a serious problem in production.

	---------------
