DONE

	Server backend is now healthy.

	google.txt needs to be adjusted to remove the ' ' between the datacenter names

	Fix server backend so it listens on port 45000

	Setup staging and production buckets

	Setup staging and production deploys

	Split apart relay and config deploys, since they are not tied to any specific branch.

	Make deploy backend automatic for dev, staging and prod branches

	Generalize deploy backend so it works on multiple branches

	Add deploy database step

	Test dev automatic deploy backend.

	Limit automatic deploy to dev and staging. Prod is manual.

	Get access to AWS account.

	Extract the full set of amazon AZIDs

		> aws ec2 describe-availability-zones --all-availability-zones

	https://docs.aws.amazon.com/ram/latest/userguide/working-with-az-ids.html

TODO

	Setup new amazon.txt

	-----------------------

	Import all amazon datacenters into dev.sql

	-----------------------

	When creating the postgres database via terraform, there needs to be something that runs create.sql and ${BRANCH}.sql to set up the initial state of the database.

	-----------------------

	Start playing around with spinning up relays in semaphore.

	First, create a new "Relays" project in google cloud.

	Add a new "relays" terraform project that points to the google cloud provider, and spins up a fixed number of relays with given configuration in dev.

	The goal here is to just spin up the relay VMs and configure them, but we'll not actually be able to modify the 

	-----------------------

	How can I setup a terraform provider to configure the database, and ultimately, database.bin?

	-----------------------








































	--------------

	I am blocked on the external UDP load balancer configuration.

	--------------

	Verify that the raspberry server inits with the server backend.

	--------------

	Verify that the raspberry client connects and upgrades

	--------------

	Verify that I can run a raspberry server locally after "next select dev" and it initializes with the dev server backend.

	Need to make sure the server backend UDP packets sent back to the server, "come from" the IP address and port of the load balancer.

	Details here: https://cloud.google.com/load-balancing/docs/network/udp-with-network-load-balancing

	This can be done by adjusting the startup script in the instance template, *OR* by forcing the server backend to bind to the load balancer address.

	--------------

	Verify that I can run raspberry clients locally and they hit the raspberry backend and connect to servers and upgrade.

	--------------




































































































	






	--------------

	Strange behavior in redis streams if I reduce the channel size from 10k to 1k. 

	Seems like multiple receives? Is it not configured correctly? Are all consumers receiving the same messages?

	Dive in and work out what the fuck is going on...

	--------------

	Get google pubsub emulator setup.

	Modify local env so the server backend actually sends messages to analytics via pubsub.

	Verify analytics actually gets each message sent to it.

	--------------

	Setup the bigquery emulator including schema.

	Write the analytics messages to bigquery emulator.

	Verify the messages get written to bigquery.

	--------------

	Extend analytics so we can specify the number of consumer threads (if it's not already).

	Make sure that analytics doesn't block forever on messages, but instead polls. If it sleeps, messages will get stuck in queues and not flushed to bigquery when we have too many consumers in analytics.

	Implement func test analytics (include both pubsub -> analytics, and analytics -> bigquery is working).

	--------------

	Research terraform and work out how to use it with google cloud.

	Implement a terraform script that brings up a dev environment in google cloud.

	--------------

	Get the portal back up with the happy path.

	Portal should talk exclusively to "api" to get what it needs.

	No auth, just hack it up to show the stuff that we need.

	--------------



















Nice to have:

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	---------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	--------------

	Extend the SDK5 so we have the option of doing near relay pings at multiple times throughout the session.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	--------------

	Functional tests to verify that relay gateway, relay backend, server backend 5 are resilient to invalid or won't read in bad database.bin files

	--------------

	There should be relay functional tests.

	---------------

	I really can't answer the question "why are half the relays not carrying sessions in dev?"

	Is it just a property of the optimizer, eg. it will go away if I disable mispredict and set all costs to zero?

	Or is there some sort of bias causing this. It seems important to explore this and work it out.

	If there is some systemic bias that just makes some relays carry a lot of sessions while other relays don't... that's a serious problem in production.

	---------------

	Leader election func tests for relay backends + ready delay

	---------------

	Extend "next relays" to show "PublicAddress" and "InternalAddress" for relays.

	---------------

	Relay gateway verify keypair on startup

	Server backend verify keypairs on startup

	---------------

	Should be func tests that spin up the bigquery emulator and verify that each analytics message type can be written to the appropriate table created from schema

	---------------

	Add internal session events. Track server hitches, client hitches, no packets received, over bandwidth etc.

	These should be passed on to both the portal for immediate visualization, and to bigquery for analysis.

	Plumb internal events all the way up from client SDK to backend.

	Client SDK must be able to trigger certain events (eg. client hitches, no packets received etc...)

	Server can trigger others, then they get OR'd before sending up to backend.

	We need a way to track no packets on client for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	We need a way to track no packets on server from session for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	--------

	Redo pro, but do it via internal session events.

	---------------------------------------------------------

	We *may* need multiple "inserters" for pubsub and redis streams, switched on by session id, so we can effectively distribute processing. 

	Otherwise, with only one inserter and one server backend (typical) it will not distribute processing across multiple portal crunchers evenly.

	Distribute according to modulo session id, or hash of server address + port, or relay id

	---------------------------------------------------------

	SDK level.

	If near relay RTT really is valid (eg. > 0.0), then it must be rounded up to 1.0, or the near relay becomes unavailable.

	This really only applies when in the LAN with the relay, but for testing, especially local testing, this is annoying and inconsistent.

	---------------------------------------------------------

	Add server sdk to link to match

	next_server_match ( match_id )

	Make sure the match id gets passed up in both the server init, server update and the session updates from SDK -> backend.

	--------

	Add uint64 type to match data function

	Extend SDK to support multiple calls to match data

	Extend SDK to store an array of match data, and flush that array

	Extend match data packets to server backend to include type

	--------------

	Match events ()

	next_server_match_event

	--------------

	Session events (address)

	next_server_session_event

	--------------

	Session data (address, type, array of doubles)

	next_server_session_data

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Implement a fix for a re-ordering of route tokens / continue tokens to create loops. Order of tokens must be enforced.

	This probably means we need to have some sort of signed bit, that indicates that indeed, this is token n, n+1 etc.

	And then this can be checked.

	Needs to be designed...

	--------------

	The SDK must never transmit the user hash in plaintext. It must always be encrypted.

	--------------

	Get "pro" tagging back in. This is a cool feature.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Multipath across two network next routes (Psyonix).

	--------------

	Atomics in SDK mean I can't just clear the whole structs. Godfucking damnit.

	--------------

	Fix compile warnings on linux, both in raspberry, and in next.cpp

	--------------

	Need to fix up the silly next/prev internal overlay w. addresses in route tokens

	--------------

	I think I can fix up the whole crazy thing with ordering of tokens by requiring that the "prev address" is specified, and is not just inferred by the address packets come in on.

	Given that the advanced packet filter will check that the packets are actually coming from the client external address already, nothing of value is lost here.

	--------------

	There are issues where cleanup does not occur in func tests due to panics.

	A systematic fix to this problem is desirable, failing this, we should do a cleanup in run.go of known processes by name, before and after running the func test locally.

	--------------

	Chaos monkey testing. The goal is to ensure no fallback to directs.

	--------------

	Any configuration in the SDK must be easily identified at the top of the file, especially bucket for google.txt, amazon.txt, multiplay.txt

	Domains for server backend

	Relay backend keypairs

	Server backend keypairs

	Customer private keys 

	etc...

	--------------

	Any configuration in the semaphore needs to be considered, eg. buckets to upload artifacts, sdk config etc.

	Secrets for accessing google cloud to upload files to google cloud storage

	--------------

	Optimize code so it doesn't run tests unless stuff has changed, eg. https://docs.semaphoreci.com/reference/conditions-reference/

	Don't waste money when only changing TODO...

	Don't allow production deploy unless on prod branch.

	The other branches, whatever...

	--------------