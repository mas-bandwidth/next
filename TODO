DONE

TODO

	Create a terraform provider for network next that drives the CRUD REST API

	https://github.com/spaceapegames/terraform-provider-example/blob/master/provider/provider.go

	https://developer.hashicorp.com/terraform/registry/modules/publish

	^--- can publish a module here

	-----------------------------------

	Good reference for documentation:

	https://github.com/hashicorp/terraform/

	-----------------------------------

	Plugin framework

	https://github.com/hashicorp/terraform-plugin-sdk

	-----------------------------------









































	Research how to get a HTTPS certificate automatically from letsencrypt.com (ideally, via terrafom...?)

	Then this needs to be plugged in to the dev terraform setup

	https://registry.terraform.io/providers/vancluever/acme/latest/docs/guides/dns-providers-cloudflare

	https://letsencrypt.org/how-it-works/

	"We recommend that most people with shell access use the Certbot ACME client. It can automate certificate issuance and installation with no downtime. It also has expert modes for people who don’t want autoconfiguration. It’s easy to use, works on many operating systems, and has great documentation. Visit the Certbot site to get customized instructions for your operating system and web server."

	https://certbot.eff.org

	https://registry.terraform.io/providers/vancluever/acme/latest

	https://itnext.io/lets-encrypt-certs-with-terraform-f870def3ce6d

	https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/certificate_pack

	https://itnext.io/lets-encrypt-certs-with-terraform-f870def3ce6d

	https://blog.bryantluk.com/post/2018/06/02/terraform-and-lets-encrypt-on-google-cloud-platform/

	https://cloud.google.com/certificate-authority-service/docs/using-terraform

	https://xbery.medium.com/say-goodbye-to-lets-encrypt-welcome-google-managed-ssl-certificates-4d92831750e1

	^------ winner. work directly with google. may need to bypass cloudflare?

	-----------------------------------

	API service must run over https in dev.

	-----------------------------------

	Relay gateway service must run over https in dev.

	-----------------------------------

	Next step has to be to actually setup some relays.

	Let's do this via a terraform provider for network next.

	-----------------------------------






















































	-----------------------

	Spin up relays with terraform and verify they are showing up in dev

	-----------------------

	Verify that I can run raspberry clients locally and they hit the raspberry backend and connect to servers and upgrade.

	-----------------------



































































































	






	--------------

	Strange behavior in redis streams if I reduce the channel size from 10k to 1k. 

	Seems like multiple receives? Is it not configured correctly? Are all consumers receiving the same messages?

	Dive in and work out what the fuck is going on...

	--------------

	Get google pubsub emulator setup.

	Modify local env so the server backend actually sends messages to analytics via pubsub.

	Verify analytics actually gets each message sent to it.

	--------------

	Setup the bigquery emulator including schema.

	Write the analytics messages to bigquery emulator.

	Verify the messages get written to bigquery.

	--------------

	Extend analytics so we can specify the number of consumer threads (if it's not already).

	Make sure that analytics doesn't block forever on messages, but instead polls. If it sleeps, messages will get stuck in queues and not flushed to bigquery when we have too many consumers in analytics.

	Implement func test analytics (include both pubsub -> analytics, and analytics -> bigquery is working).

	--------------

	Get the portal back up

	--------------



















Nice to have:

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	---------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	--------------

	Extend the SDK5 so we have the option of doing near relay pings at multiple times throughout the session.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	--------------

	Functional tests to verify that relay gateway, relay backend, server backend 5 are resilient to invalid or won't read in bad database.bin files

	--------------

	There should be relay functional tests.

	---------------

	I really can't answer the question "why are half the relays not carrying sessions in dev?"

	Is it just a property of the optimizer, eg. it will go away if I disable mispredict and set all costs to zero?

	Or is there some sort of bias causing this. It seems important to explore this and work it out.

	If there is some systemic bias that just makes some relays carry a lot of sessions while other relays don't... that's a serious problem in production.

	---------------

	Leader election func tests for relay backends + ready delay

	---------------

	Relay gateway verify keypair on startup

	Server backend verify keypairs on startup

	---------------

	Should be func tests that spin up the bigquery emulator and verify that each analytics message type can be written to the appropriate table created from schema

	---------------

	Add internal session events. Track server hitches, client hitches, no packets received, over bandwidth etc.

	These should be passed on to both the portal for immediate visualization, and to bigquery for analysis.

	Plumb internal events all the way up from client SDK to backend.

	Client SDK must be able to trigger certain events (eg. client hitches, no packets received etc...)

	Server can trigger others, then they get OR'd before sending up to backend.

	We need a way to track no packets on client for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	We need a way to track no packets on server from session for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	--------

	Redo pro, but do it via internal session events.

	---------------------------------------------------------

	We *may* need multiple "inserters" for pubsub and redis streams, switched on by session id, so we can effectively distribute processing. 

	Otherwise, with only one inserter and one server backend (typical) it will not distribute processing across multiple portal crunchers evenly.

	Distribute according to modulo session id, or hash of server address + port, or relay id

	---------------------------------------------------------

	Add server sdk to link to match

	next_server_match ( match_id )

	Make sure the match id gets passed up in both the server init, server update and the session updates from SDK -> backend.

	--------

	Add uint64 type to match data function

	Extend SDK to support multiple calls to match data

	Extend SDK to store an array of match data, and flush that array

	Extend match data packets to server backend to include type

	--------------

	Match events ()

	next_server_match_event

	--------------

	Session events (address)

	next_server_session_event

	--------------

	Session data (address, type, array of doubles)

	next_server_session_data

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Implement a fix for a re-ordering of route tokens / continue tokens to create loops. Order of tokens must be enforced.

	This probably means we need to have some sort of signed bit, that indicates that indeed, this is token n, n+1 etc.

	And then this can be checked.

	Needs to be designed...

	--------------

	The SDK must never transmit the user hash in plaintext. It must always be encrypted.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Multipath across two network next routes (Psyonix).

	--------------

	Atomics in SDK mean I can't just clear the whole structs. Godfucking damnit.

	--------------

	Fix compile warnings on linux, both in raspberry, and in next.cpp

	--------------

	Need to fix up the silly next/prev internal overlay w. addresses in route tokens

	--------------

	I think I can fix up the whole crazy thing with ordering of tokens by requiring that the "prev address" is specified, and is not just inferred by the address packets come in on.

	Given that the advanced packet filter will check that the packets are actually coming from the client external address already, nothing of value is lost here.

	--------------

	There are issues where cleanup does not occur in func tests due to panics.

	A systematic fix to this problem is desirable, failing this, we should do a cleanup in run.go of known processes by name, before and after running the func test locally.

	--------------

	Chaos monkey testing. The goal is to ensure no fallback to directs.

	--------------

	Any configuration in the SDK must be easily identified at the top of the file, especially bucket for google.txt, amazon.txt, multiplay.txt

	Domains for server backend

	Relay backend keypairs

	Server backend keypairs

	Customer private keys 

	etc...

	--------------

	Any configuration in the semaphore needs to be considered, eg. buckets to upload artifacts, sdk config etc.

	Secrets for accessing google cloud to upload files to google cloud storage

	--------------

	Optimize code so it doesn't run tests unless stuff has changed, eg. https://docs.semaphoreci.com/reference/conditions-reference/

	Don't waste money when only changing TODO...

	Don't allow production deploy unless on prod branch.

	The other branches, whatever...

	--------------

	The nice prompt for relay SSH needs to be done on ssh login via next tool

	It can't easily be done in start script for google cloud relays :(

	--------------

	Review debug endpoints on the relay gateway

	If there are any (?), make sure they can be hidden, eg. if DEBUG

	--------------

	Almost certainly need to adjust relay manage to build route matrix only including active relays

	There are just too many relays defined, esp. all google and AWS ones...

	--------------

	Remember to bring back logic to exclude near relays with significantly higher than average jitter

	--------------

	Need some way to easily deploy backend code without having to manually edit terraform.tfvars to increase the counter

	--------------

	Implement "next sessions"

	Implement "next servers"

	--------------

	Get raspberry backend, raspberry servers and raspberry clients running inside docker compose, so we can have a decent amount of activity to show by navigating to the portal URL locally.

	--------------

	Idea. "next status" gives a quick overview of the whole system, what services are up, what they are doing etc.

	--------------

	Try to simplify route shader parameters.

	RTT Veto doesn't really need multiple values.

	Latency Threshold would be much better named as "Minimum Latency Reduction" etc.

	Why have "reduce latency" and "reduce packet loss" bools at all? Of *course* you always want this... etc.

	--------------

	API should have extensive logs for actions it performs. Right now it's completely silent.

	core.Debug logs only.

	--------------

	API func tests needs to check new read item by id methods

	--------------
	
	There should be a functional test to exercise all parts of the CRUD via terraform, and check results

	--------------
