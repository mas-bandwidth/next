DONE

	Use queries to look for bad things

	Fallback to direct is happening a lot (thousands of times a day)

	Fallback to direct error is not showing up in error code.

	SessionError_Aborted = (1 << 5) seems to be happening semi-frequently (hundreds of times a day)

	Bad slice number (512) is happening extremely rarely. a few a day...

	A lot of route_no_longer_exists are happening, 1000s a day.

	Latency worse is happening less than 100 times a day.

	Mispredict is happening thousands of times a day.

	Best thing I can do is try disabling the mispredict.

	I can also disable the latency worse.

	Did this by setting core.Relax = true, easier than doing it in route shader.

	This way we can turn off relax whenever and return to old behavior.

	Verify we no longer see next latency too high

	Verify we no longer see mispredicts

	Verify we no longer see latency worse

	Look into the abort. Why does it happen? Is it related to fallback to direct?

		// currently taking network next

		if !state.Request.Next {

			// the sdk aborted this session

			core.Debug("aborted")
			state.Output.RouteState.Next = false
			state.Output.RouteState.Veto = true
			state.Error |= constants.SessionError_Aborted
			if state.Debug != nil {
				*state.Debug += "aborted\n"
			}
			return
		}

	It's basically the backend going, OK hold on, the previous slice was next, but now we're direct.

	It's effectively a fallback to direct that forgets to set the fallback to direct flag.

	To diagnose what is going on, we need to find where (on the server or client) it might
	transition from being next to being on direct, without setting the fallback to direct flag.

	Build up one graph "bad stuff" where all the bad stuff we don't want to see is graphed in the last 24 hours by hour.

	This will make it easy to apply fixes and verify they work quickly.

	The fallback to direct error is not being set in the summary, even though it is set on the bool.

	Drill in and find brasil sessions above 100ms that aren't being fixed.

	Why not? What's happening?

	Implement a session debugger tool, where I can pass in the session id and see details about it that I care about, then investigate some sessions.

	Implement a user sessions tool, so I can look at session data for a player from user hash.


TODO

	To save money I should seriously consider downsizing some google cloud and AWS relays.

	At the session counts they have, many do not need more than 2 cores.

	This could save a lot of money!

	-----------

	Sessions to look at:

		-8003309578772432229
		-2198154435340565541
		4455691751385035519
		-1441849604715854793
		-8893713270631889324
		1043069459014661946
		428870417921553802
		-8079343205311140378
		-3147773086764525781
		-7129124477292008386
		-4567202628162033718
		-5634882001556190819
		-3941993414834069369
		6536157024088213710
		-6633077465191186675
		8670609229972116695
		3641719633669816223
		2417178463301127797
		156189975710988
		983081899982361578
		-7047731294649380860
		8560327377591376115
		-4327547645019040404
		-7086141734212354562
		231475525607693085
		-403984737721862692
		3039743475755461855
		409608075494683598
		3073525261838943415
		-4424465432921318146
		2487808790524640162
		-4214265392311060780
		-2715793225429511007
		-5186463789204744276
		8675631615792893545
		-5473578955266836478
		2017066945909520883
		-6093939778817070830
		-3174920187786293718
		6633073480815668848
		6439867405407070614
		-5300094815582476982
		-7670141798708337608
		9082004402169554317
		-7318204436850403823
		-6443929599391598748
		-304939386591209377
		-5123253739640444109
		696277033156270097
		-1776092636647349025
		9044229743424711168
		6026279906091955814
		7883766532371512219
		-8583084117910481453
		-8714797941159341927
		26775334483397526

	-----

	It seems that duration_on_next is sometimes zero, even though a session has spent time on network next.

	What's going on here?!

	Example session:

		-8003309578772432229

	-----

	Sometimes the very last slice has really high latency (lag spike)

	Example sessions:

		-2198154435340565541
		4455691751385035519

	These sessions also tend to have fallback_to_direct as true on the last slice

	Did the session have a huge packet loss event, or did the person turn off their console?

	If they turned off their console, then fallback_to_direct wouldn't get set to true, because it's set to true on the client.

	So I think these are just big packet loss events causing fallback to direct.

	-----

	Here is an example of a session that had > 100ms, but we couldn't accelerate it:

		1043069459014661946

	This can happen because people are playing on a VPN.

	We should be able to confirm VPN play by looking at client relays for the session. If all client relays are high, the player is on a VPN and that's why we can't accelerate them.

	-----

	Things not fixed yet:

		1. Fallback to direct

		2. Abort

		3. Lost route

	-----

	Wait for the patch to go out. We should see a reduction in fallback to direct (longer timeouts).

	Plus, we will see the reasons for fallback to direct. Identify what is happening and think about how it could happen.

	Can we reduce it?

	-----

	Try to identify why abort is triggering.

	-----

	Why is lost route happening?

	It could be that the server relay is flaky

	It could be that the client has only one working client relay, and it goes away or becomes not routable?

	But shouldn't the system find other routes? There's usually a lot of route redundancy.

	Drill in on sessions that lost route and see what's happening.

	-----

	Fallback to direct is counter productive because it remove visibility past the point of fallback.

	Instead, we can rely on veto and abort to say, OK, we shouldn't accelerate this person anymore

	But it's good for them to keep talking to the backend as long as the session exists, so we retain visibility!

	-----




















	-----

	Would be nice if I could get the session counts over to the left a bit so I don't need to include scrollbar in screenshots...

	-----

	Add a functional test to verify that we see the correct lat/longs passed up from the SDK

	-----

	Add a functional test to verify that we see server delta time min/max/avg passed up from SDK

	-----

	Add a functional test to verify we get game rtt, jitter and pl

	-----

	Add a functional test to verify we see flags

	-----

	Add functional test to verify we see match id from sessions

	-----

	Update documentation

	-----

	Fix some easy issues

	-----

	Make release

	-----
