DONE

	Hook up the ip2location to the new server 5 backend via handler -> state

	Disable google pubsub by default. Enable with an env var.

	Disable google bigquery by default. Enable with an env var.

	Got the raspberry pi server initing with the server backend 5 in dev.

	Got the raspberry pi client connecting to the raspberry pi server and upgrading.

TODO

	For some reason the session update packet is not getting back to the server:

		19.892527: info: server upgraded client 45.33.53.242:60850 to session aa3b73f412a9d3c
		19.916452: debug: server sent session update packet to backend for session aa3b73f412a9d3c
		20.932579: debug: server sent server update packet to backend (1 sessions)
		20.932623: debug: server resent session update packet to backend for session aa3b73f412a9d3c (1)
		20.945949: debug: server received server update response packet from backend
		21.500253: debug: server received client stats packet for session aa3b73f412a9d3c
		21.989672: debug: server resent session update packet to backend for session aa3b73f412a9d3c (2)
		22.509892: debug: server received client stats packet for session aa3b73f412a9d3c
		22.972838: debug: server resent session update packet to backend for session aa3b73f412a9d3c (3)
		23.512870: debug: server received client stats packet for session aa3b73f412a9d3c
		23.993208: debug: server resent session update packet to backend for session aa3b73f412a9d3c (4)
		24.517379: debug: server received client stats packet for session aa3b73f412a9d3c
		25.009482: debug: server resent session update packet to backend for session aa3b73f412a9d3c (5)
		25.009775: error: server timed out waiting for backend response for session aa3b73f412a9d3c

    -----------------

    On the server I see:

		169.111081: debug: server received server update response packet from backend
		169.111135: debug: server updated magic: 53,83,a2,32,96,e4,f0,48 | 7c,90,55,97,42,05,a5,b8 | 7d,1e,b5,7f,6d,17,1e,94
		169.126951: debug: server current magic: 7c,90,55,97,42,05,a5,b8
		169.128136: debug: server advanced packet filter dropped packet ...
		169.853804: debug: server resent session update packet to backend for session a4c90ad0093bd17d (1)
		169.858992: debug: server advanced packet filter dropped packet ...
		170.840616: debug: server resent session update packet to backend for session a4c90ad0093bd17d (2)
		170.856632: debug: server advanced packet filter dropped packet ...
		171.810764: debug: server resent session update packet to backend for session a4c90ad0093bd17d (3)
		171.837639: debug: server advanced packet filter dropped packet ...
		172.797537: debug: server resent session update packet to backend for session a4c90ad0093bd17d (4)
		172.813502: debug: server advanced packet filter dropped packet ...
		173.880329: debug: server resent session update packet to backend for session a4c90ad0093bd17d (5)
		173.880505: error: server timed out waiting for backend response for session a4c90ad0093bd17d
		174.200015: debug: server advanced packet filter dropped packet ...
		174.200070: debug: server client ping timed out for session a4c90ad0093bd17d

	So it seems that the server is both dropping some packets from the client, due to advanced packet filter, and also not receiving the session update response from the server backend 5.

	-----------------

	On the server backend 5 I see:

		Jan 09 18:34:06 server-backend5-mig-nnj1 app[44148]: error: could not read session update request packet from 45.76.24.216:50000

	It seems that the server backend just can't read the session update request packet sent from the server.

	But why not? Is there a desync in the keypairs?

	This can't be true, because the server update and response get through just fine.

	Therefore, it must be a desync between the SDK session update packet and the backend SDK update packet.

	-----------------





























	-----------------

	Implement a new portal cruncher based around the service.go

	-----------------

	Extend server backend to write session update data messages to redis pubsub.

	-----------------

	Run the portal cruncher in the happy path and verify it gets the redis pubsub messages.

	-----------------

	Update the old database code to stash the "Latitude" and "Longitude" for datacenters in the parent struct, so it is compatible with the new database code.

	-----------------

	Implement func tests for each major service, especially around leader election and transfer in-situ

	-----------------

	We need to check the crypto signature on relay updates.

	-----------------












































    -----------------

    After this, it is time to plumb session update messages to analytics, then process them to perform insertion into bigquery.

    -----------------

    Once this is done, we basically have a working SDK5 backend in dev.

    -----------------

    The next step then is to setup a build process for relays in semaphore.

    -----------------

    Then extend the relay to have the concept of a "secondary" backend, which it will send relay updates to, but it will not shut down if it can't communicate with.

    -----------------

    Once this is done we can create a new prod5 environment, and set up a few relays in prod so that they use prod5 as secondary relay backend.

    -----------------

    Once this is verified stable, upgrade all relays so they report to the secondary prod5 relay backend.

    -----------------


































    -----------------

	analytics service is spamming this in dev:

		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping
		Jan 04 03:38:30 analytics-mig-hkq3 app[4831]: error: could not read cost_matrix_stats message - dropping

	Why can't it read the cost_matrix_stats message?

	-----------------

	analytics is failing to insert into bigquery in dev:

		Jan 04 04:29:18 analytics-mig-hkq3 app[4833]: error: failed to publish bigquery entry: 244 row insertions failed (insertion of row [insertID: "9blW809c4pZF2Tre0gkTFV5UKaI"; insertIndex: 0] failed with error: {Location: "numRelays"; Message: "no such field: numRelays."; Reason: "invalid"}, insertion of row [insertID: "IFxlFo9QjZskGR38a35pHJAoItl"; insertIndex: 1] failed with error: {Location: "numDatacenters"; Message: "no such field: numDatacenters."; Reason: "invalid"}, insertion of row [insertID: "27d1wczpzBolHFsuiwfOsUdax04"; insertIndex: 2] failed with error: {Location: "numDatacenters"; Message: "no such field: numDatacenters."; Reason: "invalid"}, ...)
		Jan 04 04:29:18 analytics-mig-hkq3 app[4833]: error: failed to publish bigquery entry: 245 row insertions failed (insertion of row [insertID: "fqcpRXzcepXDie22eUl57AyUZV8"; insertIndex: 0] failed with error: {Location: "bytes"; Message: "no such field: bytes."; Reason: "invalid"}, insertion of row [insertID: "ce23BHskKZVXVQ8U0mlDfROp5Re"; insertIndex: 1] failed with error: {Location: "numRelays"; Message: "no such field: numRelays."; Reason: "invalid"}, insertion of row [insertID: "WH8doSUiENSPxzdIxjMnA04Jq6U"; insertIndex: 2] failed with error: {Location: "numRelays"; Message: "no such field: numRelays."; Reason: "invalid"}, ...)

	-----------------
