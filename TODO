DONE

	Reproduce leader election issue locally and fix it.

	Verify leader election fix in staging

	It's better, we don't have the fighting between multiple leaders, but it seems like the new leader (or the old leader?) goes into a period where the route matrix has no routes during transition.

	So it's not really working still.

	Seems like with a longer initial delay it is working now. Keep watching though...

	Modify the server backend to perform the portal cruncher functionality in situ

	Verified session updates are coming through from server backend!

	There is a problem with fallback to directs happening in server backend.

	It seems that every session falls back to direct...

	My guess is that there is not enough throughput via the channels to push all the data to portal?

	Check that the channels are buffered properly.

	Tried increasing channel buffer size.

	See if the channel size increase works.

	Otherwise, I'll need multiple threads processing messages with sessionId % channelCount picking which channel to send down to get the throughput.

	Channel size increased worked. Keep it simple and just leave it for now, unless scaling up shows that we need to have more channels.

	Get the server update processing done in the server backend

	Get the near relay update processing done in teh server backend

	Verify near relay data shows in portal for staging sessions

	Scale up to 1M

	Does session stretching occur still without the portal cruncher?

	Not so far at 1M. Everything looks perfect.

	How does the load look on redis now?

	Definitely fewer connections...

	How does the load look on server backend?

	Looks fine.

	2.5M sessions. No stretching!

	Scale up to 5M.

	Do we have any session point stretching?

	No, its' fine!

TODO

	Scale up to 10M

	----------

	Remove portal cruncher code completely

	----------

	I think the relay backend is becoming leader before it fully has a route matrix, now and then due to timing. Investigate.

	----------

	Solve the problem with 1000 relays in staging

	----------






































































































	------------

	Active relays seems wrong. I have 1000 relays, but it shows 300.

	--------------

	Change analytics to perform bulk inserts instead of streaming inserts.

	https://cloud.google.com/bigquery/docs/write-api-batch

	--------------












Small things:

	--------------

	Just leave the buyer page as graphs. The rightmost column doesn't add enough to justify its existence.

	--------------

	Click on server to see sessions on server is broken in staging even at small load. My guess, sessions are not properly linking to server addresses?

	--------------

	Active relays seems to double during deploys. Currently, I cannot think of any reason why this would happen?

	--------------

	Extend session update so it sends all sessions to portal below 100k sessions total, even if portal next sessions only is set

	--------------

	Relay graphs need to be aggregated (time series) into per-minute values. Use average.

	--------------

	Y axis labels spill over the right side when they get up to 100,000, 1,000,000 etc... convert to 500K, 1M, 2M?

	--------------

	Current sessions on server doesn't seem to be correct, or at least, it is 181 when it should be just 1...

	--------------

	uPlot graphs really need to calculate the real maximum from the data passed in

	In so many cases, they fail to calculate it. It's extremely annoying.

	--------------

	I need more space on a standard macbook air screen in the sessions list for longer ISP names

	Right now it is way too tight. A long ISP name would throw the whole layout off

	--------------

	There will be a challenge getting prod relays up in AWS vs. dev. There are no projects to segregate?

	Might need to create a separate project, or distinguish resources with naming convention.

	--------------

	Connection type detection needs to be brought back for all platforms in the SDK. On linux, connection type was 0 -> "Unknown"

	--------------

	Session counts on relays being 8 when relay backend restarts seems a bit suspicious. Are we not decrementing session counts somewhere?

	--------------

	Probably good to provide a way to disable the high priority threads on server with env var.

	When many server instances are running on one server, this can cause problems. eg. thread starvation

	--------------










Finalize terraform and document:

	--------------

	Setup projects and service accounts with terraform

	--------------

	Relays need to be setup to use cloud storage for tf state

	--------------








Finalize SDK and UE5 plugin:

	------------------

	Update to latest PS4 and PS5 SDK on Windows PC

	Verify that we can build, link and run across PS4

	Verify that we can build, link and run across PS5

	------------------

	Update to latest XDK

	Verify that we can build, link and run across XBoxOne

	Verify that we can build, link and run across SeriesX

	------------------

	Setup PS4 compilation with custom agents

	Setup PS5 compilation with custom agents

	Setup XBoxOne compilation with custom agents

	Setup Series X compilation with custom agents

	------------------

	Get the UE5 plugin back up

	Make sure to include Flush on the server before the server is destroyed

	------------------
