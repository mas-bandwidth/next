DONE

	Print out file sizes in bytes

	New theory for fallback to direct. The sessions are going to the new instance before it is ready to service traffic.

	Evidence for this. I see fallbacks to direct happening on the old server backend, *before* it is told to shut down.

	Idea #1. Go into a server backend instance and systemctl stop it.

	This should make it go into shutdown state for 60 seconds.

	Does this trigger fallback to directs? If not, then there is something going on differently in the mig update that is causing the issue.

	On systemctl stop for server backend I see this:

		Oct 25 02:17:14 server-backend-0hs1 app[1530]: connection drain...
		Oct 25 02:17:14 server-backend-0hs1 systemd[1]: Stopping Network Next App Server...
		Oct 25 02:17:14 server-backend-0hs1 app[1530]: panic: runtime error: invalid memory address or nil pointer dereference
		Oct 25 02:17:14 server-backend-0hs1 app[1530]: [signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0xe71121]
		Oct 25 02:17:14 server-backend-0hs1 app[1530]: goroutine 26 [running]:
		Oct 25 02:17:14 server-backend-0hs1 app[1530]: github.com/oschwald/maxminddb-golang.(*Reader).Lookup(0xc0004e1040?, {0xc03c52eea0?, 0xc032bb3d40?, 0xc032bb3d40?}, {0xf36880?, 0xc03c46f970?})
		Oct 25 02:17:14 server-backend-0hs1 app[1530]:         /home/semaphore/go/pkg/mod/github.com/oschwald/maxminddb-golang@v1.12.0/reader.go:134 +0x21
		Oct 25 02:17:14 server-backend-0hs1 app[1530]: github.com/networknext/next/modules/ip2location.GetISP(...)
		Oct 25 02:17:14 server-backend-0hs1 app[1530]:         /home/semaphore/next/modules/ip2location/ip2location.go:202
		Oct 25 02:17:14 server-backend-0hs1 app[1530]: github.com/networknext/next/modules/common.(*Service).GetISP(0x118b513?, {0xc03c52eea0, 0x10, 0x10})
		Oct 25 02:17:14 server-backend-0hs1 app[1530]:         /home/semaphore/next/modules/common/service.go:319 +0xcc
		Oct 25 02:17:14 server-backend-0hs1 app[1530]: main.processPortalSessionUpdateMessages.func1()
		Oct 25 02:17:14 server-backend-0hs1 app[1530]:         /home/semaphore/next/cmd/server_backend/server_backend.go:499 +0xcf
		Oct 25 02:17:14 server-backend-0hs1 app[1530]: created by main.processPortalSessionUpdateMessages in goroutine 1
		Oct 25 02:17:14 server-backend-0hs1 app[1530]:         /home/semaphore/next/cmd/server_backend/server_backend.go:487 +0x1b9
		Oct 25 02:17:14 server-backend-0hs1 systemd[1]: app.service: Main process exited, code=exited, status=2/INVALIDARGUMENT
		Oct 25 02:17:14 server-backend-0hs1 systemd[1]: app.service: Failed with result 'exit-code'.
		Oct 25 02:17:14 server-backend-0hs1 systemd[1]: Stopped Network Next App Server.
		Oct 25 02:17:14 server-backend-0hs1 systemd[1]: app.service: Consumed 12.084s CPU time.

	Fixed with null check for isp_db and city_db

	gcloud compute instance-groups managed list-instances server-backend --region us-central1

	Verified that no fallback to direct happens with systemctl stop on a service.

	Idea #2. We need to first make the server backend instance stick around for 1hr with cooldown (potentially by adding a fake http load balancer).

	Then, while in cooldown, we can detect that we are in connection drain state on the MIG (hopefully!), and trigger a clean shutdown.

	This will let us turn the MIG upgrade so that it is exactly the same as systemctl stop.

	Confirming that with the fake http load balancer, the server backend old instances sit in DELETING for a long time.

	Confirming no fallback to direct occurs.

	Verify no fallback to directs occur with 5 minute connection drain.

	Verified!

TODO	

	Next step. Setup a fake http load balancer for the server backend.

	It can be internal, the same type as the magic backend uses. Copy the code from there, and add a 5 minute connection drain.

	Verify that we can now deploy to server backend without fallback to direct.

	----------

	Final crack at relay backend leader election in dev

	Seems like < 10 second window where no routes

	Perhaps, the new leader election takes long enough for the old entry to expire?

	----------

	Maybe the relay backend needs special code to detect "DELETING" state as well, with a connection drain -- so the instance can stop pushing leader election entries, and give leadership over to another.

	----------

	I see fallback to direct logs in server backend, but I don't see fallback to direct counters in the graphs. What's up with this?

	I need to be able to trust that the graphs capture ALL fallback to direct cases.

	----------

	I think this is a clue:

		Oct 24 23:20:27 server-backend-tg03 app[1527]: error: failed to http get route matrix: Get "http://10.0.0.20/route_matrix": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
		Oct 24 23:20:28 server-backend-tg03 app[1527]: error: failed to http get route matrix: Get "http://10.0.0.20/route_matrix": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
		Oct 24 23:20:29 server-backend-tg03 app[1527]: error: failed to http get route matrix: Get "http://10.0.0.20/route_matrix": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
		Oct 24 23:20:30 server-backend-tg03 app[1527]: error: failed to http get route matrix: Get "http://10.0.0.20/route_matrix": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
		Oct 24 23:20:31 server-backend-tg03 app[1527]: error: failed to http get route matrix: Get "http://10.0.0.20/route_matrix": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
		Oct 24 23:20:31 server-backend-tg03 app[1527]: error: http response 503 when getting route matrix
		Oct 24 23:20:32 server-backend-tg03 app[1527]: error: http response 503 when getting route matrix
		Oct 24 23:20:33 server-backend-tg03 app[1527]: error: http response 503 when getting route matrix

	Maybe it can't serve up the route matrix initally, for some reason?

	----------

	Solve the problem with 1000 relays

	----------

	Ingest into bigquery with google pubsub

	"Better yet, you no longer need to pay for data ingestion into BigQuery when using this new direct method. You only pay for the Pub/Sub you use. Ingestion from Pub/Subâ€™s BigQuery subscription into BigQuery costs $50/TiB based on read (subscribe throughput) from the subscription. This is a simpler and cheaper billing experience compared to the alternative path via Dataflow pipeline where you would be paying for the Pub/Sub read, Dataflow job, and BigQuery data ingestion. See the pricing page for details."

	https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics

	----------

	Integrate map with portal

	----------

	Relays need to be setup to use cloud storage for tf state

	----------

	For providers like akamai, vultr, amazon, we need to prefix all resource names with env (because their aren't separate projects...)

	----------

	Setup projects and service accounts with terraform

	----------






























Small things:

	--------------

	User hash link in server sessions page is not working

	--------------

	Just leave the buyer page as graphs. The rightmost column doesn't add enough to justify its existence.

	--------------

	Click on server to see sessions on server is broken in staging even at small load. My guess, sessions are not properly linking to server addresses?

	--------------

	Active relays seems to double during deploys. Currently, I cannot think of any reason why this would happen?

	--------------

	Extend session update so it sends all sessions to portal below 100k sessions total, even if portal next sessions only is set

	--------------

	Relay graphs need to be aggregated (time series) into per-minute values. Use average.

	--------------

	Y axis labels spill over the right side when they get up to 100,000, 1,000,000 etc... convert to 500K, 1M, 2M?

	--------------

	Current sessions on server doesn't seem to be correct, or at least, it is 181 when it should be just 1...

	--------------

	uPlot graphs really need to calculate the real maximum from the data passed in

	In so many cases, they fail to calculate it. It's extremely annoying.

	--------------

	I need more space on a standard macbook air screen in the sessions list for longer ISP names

	Right now it is way too tight. A long ISP name would throw the whole layout off

	--------------

	There will be a challenge getting prod relays up in AWS vs. dev. There are no projects to segregate?

	Might need to create a separate project, or distinguish resources with naming convention.

	--------------

	Connection type detection needs to be brought back for all platforms in the SDK. On linux, connection type was 0 -> "Unknown"

	--------------

	Session counts on relays being 8 when relay backend restarts seems a bit suspicious. Are we not decrementing session counts somewhere?

	--------------

	Probably good to provide a way to disable the high priority threads on server with env var.

	When many server instances are running on one server, this can cause problems. eg. thread starvation

	--------------






Finalize SDK and UE5 plugin:

	------------------

	Update to latest PS4 and PS5 SDK on Windows PC

	Verify that we can build, link and run across PS4

	Verify that we can build, link and run across PS5

	------------------

	Update to latest XDK

	Verify that we can build, link and run across XBoxOne

	Verify that we can build, link and run across SeriesX

	------------------

	Setup PS4 compilation with custom agents

	Setup PS5 compilation with custom agents

	Setup XBoxOne compilation with custom agents

	Setup Series X compilation with custom agents

	------------------

	Get the UE5 plugin back up

	Make sure to include Flush on the server before the server is destroyed

	------------------
