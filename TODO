DONE

	Implement API endpoints for relays

	Verify that I can see relay information via API during happy path

	Why are server and relay scores all set to zero? Fixed. Needed to ensure scores were uint32.

	Hook up near relay update insert in portal cruncher

	Verify that I see near relay ping data when I look up session data with happy path

	Make sure I can see near relay ping data when I get session data in happy path	

	Why are num near relays 0 in the nr-* entry?

	The data is in redis, but it is empty in the NearRelayData entry.

	Therefore, there is a parse error on near relay data

	Confirm near relay data fix.

	Fix the issue with the null database (no route matrix), make it harmless.

TODO

	Wait for leader election to complete before I run the client and server	

	----------------






















































	

	---------------------------------------------------------------------------------------------------------

	Need to explicitly wait up to 15 seconds for analytics, relay backend and api to elect a leader. 

	Otherwise, race conditions in happy path... =p

	------------------------------------------------------------

	Implement GetServerSessions -- the session ids currently connected to the server (use the hset/hgetall with the minute double buffering and merge)

	------------------------------------------------------------

	Implement GetRelayData with historical time series data

	Get the time series data for the relay, going back 10 minutes, eg. session counts, envelope bandwidth up, envelope bandwidth down, actual bandwidth up, actual bandwidth down.

	------------------------------------------------------------

	Create a new map message type in redis streams.

	This service needs leader election. It will process the map messages in redis streams and update the map.

	Actually hook up the map to run, and generate map data once per-second, store it in redis, and load the leader map.

	Serve up the leader map on "map_data" endpoint (binary).

	Implement a "map_data" endpoint in API. This will hit the map service and get the map data, then return it.

	Verify that I can get "map_data" from the API during happy path.

	------------------------------------------------------------

	We shoud shift slice number to the left one, and drop the first slice that doesn't know yet any RTTs

	This will make everything much more consistent, esp. portal and analytics that don't need to special case slice 0 RTT anymore.

	---------------------------------------------------------











































































































































































































	--------------

	Get pubsub emulator setup.

	Now actually write the analytics messages to pubsub in happy path (local).

	Verify that the analytics service receives and processes the analytics messages.

	Setup the bigquery emulator.

	Actually write the analytics messages to bigquery...

	--------

	Emit the analytics database update message from relay_backend leader.

	--------

	Add server sdk to link to match

	next_server_match ( match_id )

	Make sure the match id gets passed up in both the server init, server update and the session updates from SDK -> backend.

	--------

	Add uint64 type to match data function

	Extend SDK to support multiple calls to match data

	Extend SDK to store an array of match data, and flush that array

	Extend match data packets to server backend to include type

	--------------

	Match events ()

	next_server_match_event

	--------------

	Session events (address)

	next_server_session_event

	--------------

	Session data (address, type, array of doubles)

	next_server_session_data

	--------------

	Get the pubsub emulator back up locally

	Get the process for setting up pubsub queues etc going via "run"

	eg. "run pubsub-create", "run pubsub-destroy" or whatever works

	--------------

	Get the bigquery emulator up locally

	Decide on schemas for each of the table types, and put them under "bigquery"

	eg. "run bigquery-create", "run bigquery-destroy" or whatever.

	--------------

	Extend analytics service to insert messages into bigquery.

	--------------

	Implement simple "api" service

	--------------









































Essentials:

	--------------

	The relay should talk to the relay backend over SSH in dev and production.

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Implement a fix for a re-ordering of route tokens / continue tokens to create loops. Order of tokens must be enforced.

	This probably means we need to have some sort of signed bit, that indicates that indeed, this is token n, n+1 etc.

	And then this can be checked.

	Needs to be designed...

	--------------

	The SDK must never transmit the user hash in plaintext. It must always be encrypted.

	--------------

	Get "pro" tagging back in. This is a cool feature.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Multipath across two network next routes (Psyonix).

	--------------





Nice to have:

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	---------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	--------------

	Extend the SDK5 so we have the option of doing near relay pings at multiple times throughout the session.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	--------------

	Functional tests to verify that relay gateway, relay backend, server backend 5 are resilient to invalid or won't read in bad database.bin files

	--------------

	There should be relay functional tests.

	---------------

	I really can't answer the question "why are half the relays not carrying sessions in dev?"

	Is it just a property of the optimizer, eg. it will go away if I disable mispredict and set all costs to zero?

	Or is there some sort of bias causing this. It seems important to explore this and work it out.

	If there is some systemic bias that just makes some relays carry a lot of sessions while other relays don't... that's a serious problem in production.

	---------------

	Leader election func tests for relay backends + ready delay

	---------------

	Extend "next relays" to show "PublicAddress" and "InternalAddress" for relays.

	---------------

	Relay gateway verify keypair on startup

	Server backend verify keypairs on startup

	---------------

	Should be func tests that spin up the bigquery emulator and verify that each analytics message type can be written to the appropriate table created from schema

	---------------

	Add internal session events. Track server hitches, client hitches, no packets received, over bandwidth etc.

	These should be passed on to both the portal for immediate visualization, and to bigquery for analysis.

	Plumb internal events all the way up from client SDK to backend.

	Client SDK must be able to trigger certain events (eg. client hitches, no packets received etc...)

	Server can trigger others, then they get OR'd before sending up to backend.

	We need a way to track no packets on client for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	We need a way to track no packets on server from session for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	--------

	Redo pro, but do it via internal session events.

	---------------------------------------------------------

	We *may* need multiple "inserters" for pubsub and redis streams, switched on by session id, so we can effectively distribute processing. 

	Otherwise, with only one inserter and one server backend (typical) it will not distribute processing across multiple portal crunchers evenly.

	Distribute according to modulo session id, or hash of server address + port, or relay id

	---------------------------------------------------------
