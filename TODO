DONE

	Modified setup_relay.sh so it doesn't run more than once, in case on certain suppliers it runs each time the machine starts.

	When a relay is initially setup via startup script, it should limit SSH to the VPN only. Otherwise, it's a period of lower security before setup is applied.

	There is a problem with the terraform scirpt. If you delete all relays from a supplier and try to terraform apply, it can't work, because it no longer knows about the seller.

	The seller is derived from the relays. Now there are no relays for that seller, so the seller goes away. How to fix?

	Fixed by not deriving seller list from set of relays.

	Server is stuck in a loop. It cannot initialize with the server backend. Not sure why not.

	The buyer has the wrong id. Check the customer public keys.

	Fixed. The relays are now carrying sessions!

	Make the "next setup [relay]" re-entrant, eg. if a relay is already setup, quit.

	Need a way to get errors, especially validation errors down to the next tool as text.

	Diving in to the API ssh console is not the way to debug why the database doesn't validate...

	Extend the database response to return a json object, with "database" (base64 string), and "error"

	Then extend next tool to check the error, and print out the error if it is not "".

	Extract the git user and email and print on "next commit"

	Implement the admin API commit to accept a "user" string and a "database_base64" in JSON, create the database, verify it, and return "error" if it doesn't verify.

	Damnit I'm going to need to adjust permissions so I can write to the database.bin directory from the terraform service account...

	Extend next tool to pass up the database commit request.

	Each service instance to have the git tag embedded and known, and it should be included in the status output and console TTY for the service when it runs.

	The API "pong" could return a pong, and also the build tag, or the dev if no tag is applied (eg. local builds)

	This would provide a nice confirmation that the tag that you have deployed is live.

	API func test needs to test the "admin/database" method

	Tidy up env in next.go

	For bare metal relays, "next setup" must detect that the initial setup has not run, and perform it manually.

	Make sure that all relays "come back" after reboot. If not, why not?

	They didn't but it was just chance. The redis is out of memory:

	error: failed to send batched pubsub messages to redis: OOM command not allowed when used memory > 'maxmemory'.

	Increasing instance size to 10G.

	vultr resources did not come back, because they didn't have a fixed IP.

	Turning back on the reserved IP thing w. vultr.

TODO

	Get the portal working again

	---------------









	-----------------------------------

	API service must run over https in dev.

	https://xbery.medium.com/say-goodbye-to-lets-encrypt-welcome-google-managed-ssl-certificates-4d92831750e1

	-----------------------------------

	Relay gateway service must run over https in dev.

	-----------------------------------













	--------------

	Strange behavior in redis streams if I reduce the channel size from 10k to 1k. 

	Seems like multiple receives? Is it not configured correctly? Are all consumers receiving the same messages?

	Dive in and work out what the fuck is going on...

	--------------

	Get google pubsub emulator setup.

	Modify local env so the server backend actually sends messages to analytics via pubsub.

	Verify analytics actually gets each message sent to it.

	--------------

	Setup the bigquery emulator including schema.

	Write the analytics messages to bigquery emulator.

	Verify the messages get written to bigquery.

	--------------

	Extend analytics so we can specify the number of consumer threads (if it's not already).

	Make sure that analytics doesn't block forever on messages, but instead polls. If it sleeps, messages will get stuck in queues and not flushed to bigquery when we have too many consumers in analytics.

	Implement func test analytics (include both pubsub -> analytics, and analytics -> bigquery is working).

	--------------

	Get the portal back up

	--------------



















Nice to have:

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	--------------

	Extend the SDK so we have the option of doing near relay pings at multiple times throughout the session.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	---------------

	Leader election func tests for relay backends + ready delay

	---------------

	Should be func tests that spin up the bigquery emulator and verify that each analytics message type can be written to the appropriate table created from schema

	---------------

	Add internal session events. Track server hitches, client hitches, no packets received, over bandwidth etc.

	These should be passed on to both the portal for immediate visualization, and to bigquery for analysis.

	Plumb internal events all the way up from client SDK to backend.

	Client SDK must be able to trigger certain events (eg. client hitches, no packets received etc...)

	Server can trigger others, then they get OR'd before sending up to backend.

	We need a way to track no packets on client for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	We need a way to track no packets on server from session for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	--------

	Redo pro, but do it via internal session events.

	---------------------------------------------------------

	We *may* need multiple "inserters" for pubsub and redis streams, switched on by session id, so we can effectively distribute processing. 

	Otherwise, with only one inserter and one server backend (typical) it will not distribute processing across multiple portal crunchers evenly.

	Distribute according to modulo session id, or hash of server address + port, or relay id

	---------------------------------------------------------

	Add server sdk to link to match

	next_server_match ( match_id )

	Make sure the match id gets passed up in both the server init, server update and the session updates from SDK -> backend.

	--------

	Add uint64 type to match data function

	Extend SDK to support multiple calls to match data

	Extend SDK to store an array of match data, and flush that array

	Extend match data packets to server backend to include type

	--------------

	Match events ()

	next_server_match_event

	--------------

	Session events (address)

	next_server_session_event

	--------------

	Session data (address, type, array of doubles)

	next_server_session_data

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Multipath across two network next routes.

	--------------

	There are issues where cleanup does not occur in func tests due to panics.

	A systematic fix to this problem is desirable, failing this, we should do a cleanup in run.go of known processes by name, before and after running the func test locally.

	--------------

	Chaos monkey testing in staging. The goal is to ensure no fallback to directs.

	--------------

	Review debug endpoints on the relay gateway

	If there are any (?), make sure they can be hidden, eg. if DEBUG

	--------------

	Almost certainly need to adjust relay manage to build route matrix only including active relays

	There are just too many relays defined, esp. all google and AWS ones...

	--------------

	Remember to bring back logic to exclude near relays with significantly higher than average jitter

	--------------

	Implement "next sessions"

	Implement "next servers"

	--------------

	Get raspberry backend, raspberry servers and raspberry clients running inside docker compose, so we can have a decent amount of activity to show by navigating to the portal URL locally.

	--------------

	Idea. "next status" gives a quick overview of the whole system, what services are up, what they are doing etc.

	--------------

	Try to simplify route shader parameters.

	RTT Veto doesn't really need multiple values.

	Latency Threshold would be much better named as "Minimum Latency Reduction" etc.

	Why have "reduce latency" and "reduce packet loss" bools at all? Of *course* you always want this... etc.

	--------------

	Lumen has VMs and bare metal via terraform!!!

	https://registry.terraform.io/providers/LumenTech/lumen/latest/docs/guides/getting_started

	-----------------------------------

	Relay name should get validated to have the datacenter name substring in it

	-----------------------------------

	XBox preferred port. Do we need to do anything?

	https://learn.microsoft.com/en-us/gaming/gdk/_content/gc/networking/overviews/game-mesh/preferred-local-udp-multiplayer-port-networking

	-----------------------------------

	Idea. Accumulated event flags, per-session summary.

	Allows to quickly look at sessions that experienced certain events vs. not, without needing to look at slices.

	----------------------

	Build relay binaries for both x64 and arm, and then autodetect in scripts via OS which one is needed.

	The relay can have its "platform" set to a free form string, eg. "x64" or "arm64", then we postfix it to the relay version.

	eg. relay-debug-1.0.20-x64

	Then, each supplier can just return the "platform" var with each relay and this will be automatic and we suppoart both x86 and arm

	---------------

	Add "platform" relay property to the database schema (optional)

	Add platform relay property to admin.go

	Add platform relay property to next.go (print it out on "next relays")

	Add platform relay property to extract database (print it out on "next database")

	Implement google terraform to detect platform and return it

	Implement akamai terraform to return platform "x64" hardcoded.

	Implement vultr terraform to return platform "x64" hardcoded.

	---------------

	Work out how to cross compile relay to arm64

	Update relay release process build release/debug and x64/arm64 platforms

	Extend relay startup script to pick relay binary according to version AND platform

	Extend relay upgrade version 

	---------------
