DONE

	Cost matrix is all 255s

	All relays are considered active, or almost all.

	Relay data shows that most relays are online. Very few are offline.

	Idea. Capture the gob data for the relay manager and explore it locally. I really just don't have any idea WHY the cost matrix is all red. This may help me find out why.

	Load the relay manager gob data

	Look for patterns in the data. It seems that last update times are really scatterred. eg. I see up to 13 seconds delta between oldest and newest entries.

	This is likely the problem. If the relay updates are coming in late, due to relay gateway redis being overloaded, then this would explain what's going on.

	To check that this is the case, manually go across all source and dest samples post load, and adjust to a fixed timestamp.

	Then go "GetCosts" from the fixed timestamp. -- Costs should work and be all green. Output to html file and verify.

	The plot thickens, excluding last update time effects, the data is all set to 100000000000000

	In this case, it means that the history buffer hasn't been filled with 60 samples yet. If everything is overloaded and not all relays are getting updates, it's clear that this could be the case.

	Try excluding the history buffer effects on rtt by taking min... see what happens

	No effect. The history buffer has been cleared.

TODO	

	What is going on is that dest entries are timing out (10 seconds), because they are stale.

	This removes all RTT entries and stuff doesn't work right.

	It's definitely pointing to something being overloaded so that relay updates arrive late, and are continually timed out.

	--------

	Extend relay gateway and relay backend to check if relay updates are "late" (eg. older than 5 seconds?)

	Print out if they are late, and drop them. There is no point processing late updates. This will also give some indication that something is wrong.

	--------























































	----------

	Relay backend in staging needs a quick fix:

		Oct 25 21:29:11 relay-backend-kvdz app[2115]: error: relay insert error: dial tcp 127.0.0.1:6379: connect: connection refused

	It's still pointing at default redis...

	----------

	Ingest into bigquery with google pubsub

	"Better yet, you no longer need to pay for data ingestion into BigQuery when using this new direct method. You only pay for the Pub/Sub you use. Ingestion from Pub/Subâ€™s BigQuery subscription into BigQuery costs $50/TiB based on read (subscribe throughput) from the subscription. This is a simpler and cheaper billing experience compared to the alternative path via Dataflow pipeline where you would be paying for the Pub/Sub read, Dataflow job, and BigQuery data ingestion. See the pricing page for details."

	https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics

	----------

	Integrate map with portal

	----------

	Relays need to be setup to use cloud storage for tf state

	----------

	For providers like akamai, vultr, amazon, we need to prefix all resource names with env (because their aren't separate projects...)

	----------

	Setup projects and service accounts with terraform

	----------






























Small things:

	--------------

	Clean up old ip2location database files

	--------------

	User hash link in server sessions page is not working

	--------------

	Just leave the buyer page as graphs. The rightmost column doesn't add enough to justify its existence.

	--------------

	Click on server to see sessions on server is broken in staging even at small load. My guess, sessions are not properly linking to server addresses?

	--------------

	Active relays seems to double during deploys. Currently, I cannot think of any reason why this would happen?

	--------------

	Extend session update so it sends all sessions to portal below 100k sessions total, even if portal next sessions only is set

	--------------

	Relay graphs need to be aggregated (time series) into per-minute values. Use average.

	--------------

	Y axis labels spill over the right side when they get up to 100,000, 1,000,000 etc... convert to 500K, 1M, 2M?

	--------------

	Current sessions on server doesn't seem to be correct, or at least, it is 181 when it should be just 1...

	--------------

	uPlot graphs really need to calculate the real maximum from the data passed in

	In so many cases, they fail to calculate it. It's extremely annoying.

	--------------

	I need more space on a standard macbook air screen in the sessions list for longer ISP names

	Right now it is way too tight. A long ISP name would throw the whole layout off

	--------------

	There will be a challenge getting prod relays up in AWS vs. dev. There are no projects to segregate?

	Might need to create a separate project, or distinguish resources with naming convention.

	--------------

	Connection type detection needs to be brought back for all platforms in the SDK. On linux, connection type was 0 -> "Unknown"

	--------------

	Session counts on relays being 8 when relay backend restarts seems a bit suspicious. Are we not decrementing session counts somewhere?

	--------------

	Probably good to provide a way to disable the high priority threads on server with env var.

	When many server instances are running on one server, this can cause problems. eg. thread starvation

	--------------






Finalize SDK and UE5 plugin:

	------------------

	Update to latest PS4 and PS5 SDK on Windows PC

	Verify that we can build, link and run across PS4

	Verify that we can build, link and run across PS5

	------------------

	Update to latest XDK

	Verify that we can build, link and run across XBoxOne

	Verify that we can build, link and run across SeriesX

	------------------

	Setup PS4 compilation with custom agents

	Setup PS5 compilation with custom agents

	Setup XBoxOne compilation with custom agents

	Setup Series X compilation with custom agents

	------------------

	Get the UE5 plugin back up

	Make sure to include Flush on the server before the server is destroyed

	------------------
