DONE

	Increased batch duration to 1.1 seconds in dev. This should ensure that generally, all relay updates come in in one message, once per-second.

	Logs in the relay backend showing pings between relays demonstrate that the pings are going just fine.

	There must be something wrong in the relay manager building the cost matrix.

	gcloud compute ssh --zone "us-central1-a" "relay-backend-mig-6ssb"  --project "network-next-v3-dev"

	Radically gutted the relay manager so it's not totally stupid.

TODO

	Goal is to get the relay manager working in local first, before deploying it to production.

	Just have to update the reference relay to support all packet types again, and it's ready to go.


















	------------

	Extend the relay init to return the internal address of the relay so it can be checked.

	Extend the runtime logic to only allow usage of the internal address, if BOTH relays have an internal address, and they are the same supplier.

	This is because if we are sending to an internal address we need to encode relative to OUR internal address, and otherwise the sender won't know it.

	Verify that we get routes across google in dev.

	Bring all the relays back up. Everything should be working again.

	Make sure that the client 














	---------------

	When a relay initializes, it should fully verify it is able to work,

	eg. verify UDP port is working

	Verify that it can read/write test tokens sent to it from the backend

	Verify that it is configured with the correct public and internal address

	---------------

	I'd like to see leader election state as a nice table under "/leader"

	---------------

	Would be nice to see the database as html tables

	---------------

	I really want an easy way to see the cost matrix as html

	---------------

	next ssh google.lasvegas hangs. previously, it would pattern match...

	---------------

	Actually implement database validation

	-------------

	Simplify the database.bin updates so the service.go is responsible for pulling down from google cloud storage

	-------------

	Simplify the sync service so it just periodically downloads files and puts them in google cloud storage locations

	-------------

	Implement an extremely simple portal service that just provides the necessary endpoints + auth needed to get the backend of the next tool working.

	-------------

	Extend "next relays" to show "PublicAddress" and "InternalAddress" for relays.

	I really want to eyeball that relays have the correct internal addresses, dynamically from the relay backend.

	-------------

	Standardize the code to read from SQL into the database.go module.

	Add correct error handling, eg. return error with detailed description of what went wrong.

	Call into the database function from the extract_database tool.

	----------------


































































Essentials:

    --------------

    Send portal messages to redis streams

	--------------

	Portal cruncher

	--------------

	Send rest of server backend messages to google pubsub

	--------------

	Extend analytics service to insert messages into bigquery.

	--------------

	Leader election needs to wait at least 11 seconds to make sure it gets the correct result when > 2 vms start at the same time without flap

	--------------

	Leader election func testing for relay backends + ready delay

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Extend the relay to support a secondary relay backend.

	This will enable relays to be shared between dev and dev5, and prod and prod5.

	--------------

	We need a new relay implementation. The current relay is garbage.

	--------------








Nice to have:

	-----------

	Add match id to both portal data and session update data by default (first slice only, and summary...)

	Then work out a new API to link the session with a match (start of session, with tags...), separate from setting match data (end of match).

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	--------------

	Create a new message for near relay stats.

	Unit test it, then on slice #1 when near relay data comes in, write a near relay ping stats message and send it.

	--------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	--------------

	Implement a fix for a re-ordering of route tokens / continue tokens to create loops. Order of tokens must be enforced.

	This probably means we need to have some sort of signed bit, that indicates that indeed, this is token n, n+1 etc.

	And then this can be checked.

	Needs to be designed...

	--------------

	Extend the SDK5 so we have the option of sending down new near relay stats on multiple slices, later on.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Multipath across two network next routes.

	--------------

	It would be good to track when sessions shut down, vs. having them always time out

	This would require new messages up then back down the chain of relays for ack

	It would also reduce the risk of missing packets or having sessions time out and not noticing.

	We can now track sessions timing out before being closed, and this would indicate problems (or at least, hard disconnects...)

	Again, adding this later would be challenging. Best to do it now.

	--------------

	Rework the reference relay and make it the official relay.

	The trick is to use the code from the proxy to go wide across n threads with SO_REUSEPORT

	Since each port/address maps to a specific socket, there is no need for locks across the session map, there will be a session map per-thread.

	Stats can be managed with atomics, eg. count of sessions per-thread in a uint64 atomic, summed by the main thread before uploading the total to the relay backend.

	--------------

	A relay which has its firewall misconfigured will cause players to fallback to direct, because traffic can't be sent to it.

	Would be nice if the relay backend would talk to the relay over its UDP port and say hi, and if this heartbeat was a prerequisite for the relay coming online.

	This way misconfigured relays can't cause problems.

	--------------

	The SDK must never transmit the user hash in plaintext. It must always be encrypted.

	--------------

	Put "MatchId" into session data (first).

	This way we can easily look up from a session the match it is in.

	--------------

	I'm not sure the expire timestamp in the server backend was even working correctly.

	Investigate how the expire timestamp was used, calculated and if it was even doing anything...

	Also investigate relay concept of time in reference vs. real relay

	--------------

	Relay stats message needs to be revisited. Make sure it doesn't have old crap in it

	--------------

	We are very close to removing libsodium from golang. It should be possible. If we do this, it becomes much easier to build and run golang services, as they are fully native.

	--------------

	We need to track client no packets in 0.1 sec, 0.25sec, 0.5sec, 1.0 sec.

	This way we can identify if we are likely seeing thread starvation on the server

	They currently have 1000 server threads on the server, and this is potentially causing serious starvation issues with packets... (!!!!)

	--------------

	Can we do something to detect thread starvation on the server and warn?

	It looks like we've been in production with thread starvation with Blue Mammoth for a while now, and it would be much better if we could catch this ourselves, before it becomse a customer problem.

	--------------

	Relays should detect and count when they can't read the route token

	This indicates a misconfigured relay. This should get loaded up into bigquery so it can be searched on by data science.

	--------------

	Add confirmation for all "next" tool actions that modify relays... especially "reboot", "upgrade", "start", "stop" and so on.

	OR, just disable these tool actions in prod.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	--------------

	New relay really needs a series of counters that get updated with each update sent to the backend.

	These counters can be for bad things that we want to track, so we have the data by timestamp inserted into bigquery for analysis.

	eg. could not read route token, for example.

	--------------

	Functional tests to verify that relay gateway, relay backend, server backend 5 are resilient to invalid or won't read in database.bin files being injected.

	--------------

	There should be relay functional tests.

	--------------

	There should be unit tests, functional load tests for the relay manager.

	It's a problematic component. It must be absolutely solid.

	---------------

	Dest relays array in "relay_data" endpoint under relay backend seems wrong. There is only one dest relay configured in dev.

	{"relay_ids":["9ba2cbdac486d8b9","2decf82716faa354","8d38d21c576e920f","926fb457f1831bd3","0304a2911b4d754a","357632dc51bc4049","d2a3fc2f8c6474a6","b79d96976666501a","82dbcde5b0d364db","e35628f6f947c1bb","d2b09a62ae24aaa4","9e8bb41dd08c2b89","659b040933ce5273","25a2db96c916b189","a2949f2a602760f9","aaf8b7f147b74d8f","3017ec1b391cf4aa","eed41accabdc243d","79d7a3c16f4c7f0e","547c75c6d0df92ff","4bb32e2f5c88bc23","6283f7053b0f6ac2","7560a6e1178288e1","2561d0afd37abefd","d6dfd7505907ddc9","4531f9f6b7567736"],"relay_names":["amazon.ohio.2","amazon.oregon.1","amazon.sanjose.1","amazon.saopaulo.1","amazon.virginia.1","google.iowa.1","google.lasvegas.1","google.oregon.2","google.saltlakecity.1","google.santiago.1","google.saopaulo.1","google.virginia.3","linode.atlanta","linode.dallas","linode.fremont","linode.newark","linode.toronto","vultr.atlanta","vultr.chicago","vultr.dallas","vultr.honolulu","vultr.losangeles","vultr.miami","vultr.newyork","vultr.seattle","vultr.siliconvalley"],"relay_addresses":["3.138.73.252:40000","44.242.70.57:40000","52.52.246.62:40000","54.94.14.133:40000","34.232.104.206:40000","35.226.96.92:40000","34.125.125.84:40000","34.168.209.101:40000","34.106.29.193:40000","34.176.85.20:40000","34.151.248.241:40000","35.236.236.4:40000","45.79.196.195:40000","173.255.196.49:40000","45.33.40.47:40000","23.239.15.72:40000","172.105.104.156:40000","144.202.26.241:40000","45.76.24.216:40000","144.202.68.72:40000","208.83.233.36:40000","149.248.16.231:40000","45.32.160.85:40000","45.76.6.145:40000","144.202.82.197:40000","149.28.197.73:40000"],"relay_latitudes":[40.4173,45.8399,37.3387,-23.5558,39.0438,41.2619,36.1716,45.8399,40.7608,-33.4489,-23.5558,39.0438,33.7488,32.7767,37.3387,40.7357,43.6532,33.7488,41.8781,32.7767,21.3099,34.0522,25.7617,40.7128,47.6062,37.3387],"relay_longitudes":[-82.9071,-119.7006,-121.8853,-46.6396,-77.4874,-95.8608,-115.1391,-119.7006,-111.891,-70.6693,-46.6396,-77.4874,-84.3877,-96.797,-121.8853,-74.1724,79.3832,-84.3877,-87.6298,-96.797,-157.8581,118.2437,-80.1918,-74.006,-122.3321,-121.8853],"relay_datacenter_ids":["02853668dc46a6f6","c89b5dbc8d89e884","38ee6dfec67a834f","49da86600d3c5830","5252126480cdc627","aedb4f6e4bb13649","f144b4593aeba043","9597d52c9cea4898","35389fa2489ae96d","5dcf985626327adb","4c6f653055190935","21e3991fc0b90f58","e242dd1838980b83","7e239f41492187ef","a5143661fd16780d","fe220eaa065b5a9e","f43cd63b335556a1","90156e982b195d63","6c70929c8bf3a09c","e581423cdf2b3f4f","7e59fe8dbe4fcee6","7c4219e77b54f887","204d63ce47908a33","2fe32c22450fb4c9","f743d521e31ae508","2ac05171c4090bc6"],"relay_id_to_index":["9ba2cbdac486d8b9 - 0","2decf82716faa354 - 1","8d38d21c576e920f - 2","926fb457f1831bd3 - 3","0304a2911b4d754a - 4","357632dc51bc4049 - 5","d2a3fc2f8c6474a6 - 6","b79d96976666501a - 7","82dbcde5b0d364db - 8","e35628f6f947c1bb - 9","d2b09a62ae24aaa4 - 10","9e8bb41dd08c2b89 - 11","659b040933ce5273 - 12","25a2db96c916b189 - 13","a2949f2a602760f9 - 14","aaf8b7f147b74d8f - 15","3017ec1b391cf4aa - 16","eed41accabdc243d - 17","79d7a3c16f4c7f0e - 18","547c75c6d0df92ff - 19","4bb32e2f5c88bc23 - 20","6283f7053b0f6ac2 - 21","7560a6e1178288e1 - 22","2561d0afd37abefd - 23","d6dfd7505907ddc9 - 24","4531f9f6b7567736 - 25"],"dest_relays":["1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1"],"dest_relay_names":null}

	