DONE

	Fix hex id bug in terraform provider -> v5.0.2

	Work with maps not lists.

	This means don't work with seller_names and datacenter_names as drivers, but instead do for_each over the maps.
	
	Now we need to extend each generated.tf per-supplier to add the following per-datacenter map entry:

		1. seller_name --> eg. "google" for all google relays
		2. native_name --> eg. zone name, for google cloud.
		3. latitude
		4. longitude

	Now I need to create the actual relays. It should be easy...

	Almost there, but the structure of the relays map returned by supplier terraform needs to be adjusted to keep the port number separate from addresses.

	Change each supplier to return exactly the data as needed by the relay database entry:

		public_ip
		public_port
		internal_ip
		internal_port
		internal_group
		ssh_ip
		ssh_port
		ssh_user

	Database relays are now successfully setup from terraform!

	Setup raspberry customer

	Setup raspberry route shader

	Setup raspberry buyer

	Test that once the database is setup that I can SSH into each dev relay via "next ssh"

	I can, but amazon requires the VPN be active, even before the relays are provisioned.

	How to provision the relays?

	Initially I'm going to start with "next setup [relay]"

	This will SSH in and run a custom script with data gathered from the database, so the relay is configured as specified in terraform.

	Once this basic setup is working and reliable, then we can try to automate it further.

	Added setup command.

	Relays are all setup. We just need to push the database.bin and it should be working.

	But when I go "next database" it doesn't print out relays.

	What's going on?

	I think the database.bin that is being copied down is the committed database, instead of the database extracted from the SQL.

	What I really need to see when working with the next tool is the database as per-the SQL.

	Removed buyer keypairs from terraform plugin and REST API. We simply don't need to track buyer keypairs as a resource.

	Only AWS relays need the internal group to be set to the region (or in the case of local zones... the local zone itself, presumably)

	Google doesn't need this treatment. Set the internal group to ""

	Extend "next database" to always delete the cached database and grab it again on env change

	Extend admin REST API to have a database method.

	Call this admin database method when we "getDatabase".

	The "next" tool works from the SQL. The runtime works from the database.bin

	Verified that I can now go "next select dev" and "next database" and it prints the contents of the SQL db.

	Set up destination datacenters for the raspberry buyer.

TODO

	Amazon relay internal groups don't appear to be set correctly. Each amazon relay should have its region set as internal group, since they
	can only send packets via internal addresses within their region.

	---------------

	Need a way to get errors, especially validation errors down to the next tool as text.

	Diving in to the API ssh console is not the way to debug why the database doesn't validate...

	---------------

	Implement "next commit"

	Extend the admin API to have a "PUT" method called "commit"

	This takes the database.bin in the PUT data, parses it, validates it, then uploads to google cloud storage for the env.

	IMPORTANT: This method also needs a way to return validation errors down to the caller as text to be printed out from the next tool.

	---------------

	Extend 'next' tool env to include:

		VPNAddress
		RelayBackendAddress
		RelayBackendPublicKey

	---------------

	Terraform func test needs to create enough resources of each type to avoid hitting the hex issue I just had.

	----------

	Each service instance to have the git tag embedded and known, and it should be included in the status output and console TTY for the service when it runs.

	----------

	The API "pong" could return a pong, and also the build tag, if it's not set to "" (eg. in a local build)

	This would provide a nice confirmation that the tag that you have deployed is live.

	----------






	----------------------

	Research how to get a HTTPS certificate automatically from letsencrypt.com (ideally, via terrafom...?)

	Then this needs to be plugged in to the dev terraform setup

	https://registry.terraform.io/providers/vancluever/acme/latest/docs/guides/dns-providers-cloudflare

	https://letsencrypt.org/how-it-works/

	"We recommend that most people with shell access use the Certbot ACME client. It can automate certificate issuance and installation with no downtime. It also has expert modes for people who don’t want autoconfiguration. It’s easy to use, works on many operating systems, and has great documentation. Visit the Certbot site to get customized instructions for your operating system and web server."

	https://certbot.eff.org

	https://registry.terraform.io/providers/vancluever/acme/latest

	https://itnext.io/lets-encrypt-certs-with-terraform-f870def3ce6d

	https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs/resources/certificate_pack

	https://itnext.io/lets-encrypt-certs-with-terraform-f870def3ce6d

	https://blog.bryantluk.com/post/2018/06/02/terraform-and-lets-encrypt-on-google-cloud-platform/

	https://cloud.google.com/certificate-authority-service/docs/using-terraform

	https://xbery.medium.com/say-goodbye-to-lets-encrypt-welcome-google-managed-ssl-certificates-4d92831750e1

	^------ winner. work directly with google. may need to bypass cloudflare?

	-----------------------------------

	API service must run over https in dev.

	-----------------------------------

	Relay gateway service must run over https in dev.

	-----------------------------------













	--------------

	Strange behavior in redis streams if I reduce the channel size from 10k to 1k. 

	Seems like multiple receives? Is it not configured correctly? Are all consumers receiving the same messages?

	Dive in and work out what the fuck is going on...

	--------------

	Get google pubsub emulator setup.

	Modify local env so the server backend actually sends messages to analytics via pubsub.

	Verify analytics actually gets each message sent to it.

	--------------

	Setup the bigquery emulator including schema.

	Write the analytics messages to bigquery emulator.

	Verify the messages get written to bigquery.

	--------------

	Extend analytics so we can specify the number of consumer threads (if it's not already).

	Make sure that analytics doesn't block forever on messages, but instead polls. If it sleeps, messages will get stuck in queues and not flushed to bigquery when we have too many consumers in analytics.

	Implement func test analytics (include both pubsub -> analytics, and analytics -> bigquery is working).

	--------------

	Get the portal back up

	--------------



















Nice to have:

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	---------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	--------------

	Extend the SDK so we have the option of doing near relay pings at multiple times throughout the session.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	--------------

	Functional tests to verify that relay gateway, relay backend, server backend 5 are resilient to invalid or won't read in bad database.bin files

	---------------

	I really can't answer the question "why are half the relays not carrying sessions in dev?"

	Is it just a property of the optimizer, eg. it will go away if I disable mispredict and set all costs to zero?

	Or is there some sort of bias causing this. It seems important to explore this and work it out.

	If there is some systemic bias that just makes some relays carry a lot of sessions while other relays don't... that's a serious problem in production.

	---------------

	Leader election func tests for relay backends + ready delay

	---------------

	Should be func tests that spin up the bigquery emulator and verify that each analytics message type can be written to the appropriate table created from schema

	---------------

	Add internal session events. Track server hitches, client hitches, no packets received, over bandwidth etc.

	These should be passed on to both the portal for immediate visualization, and to bigquery for analysis.

	Plumb internal events all the way up from client SDK to backend.

	Client SDK must be able to trigger certain events (eg. client hitches, no packets received etc...)

	Server can trigger others, then they get OR'd before sending up to backend.

	We need a way to track no packets on client for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	We need a way to track no packets on server from session for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	--------

	Redo pro, but do it via internal session events.

	---------------------------------------------------------

	We *may* need multiple "inserters" for pubsub and redis streams, switched on by session id, so we can effectively distribute processing. 

	Otherwise, with only one inserter and one server backend (typical) it will not distribute processing across multiple portal crunchers evenly.

	Distribute according to modulo session id, or hash of server address + port, or relay id

	---------------------------------------------------------

	Add server sdk to link to match

	next_server_match ( match_id )

	Make sure the match id gets passed up in both the server init, server update and the session updates from SDK -> backend.

	--------

	Add uint64 type to match data function

	Extend SDK to support multiple calls to match data

	Extend SDK to store an array of match data, and flush that array

	Extend match data packets to server backend to include type

	--------------

	Match events ()

	next_server_match_event

	--------------

	Session events (address)

	next_server_session_event

	--------------

	Session data (address, type, array of doubles)

	next_server_session_data

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Multipath across two network next routes.

	--------------
	--------------

	There are issues where cleanup does not occur in func tests due to panics.

	A systematic fix to this problem is desirable, failing this, we should do a cleanup in run.go of known processes by name, before and after running the func test locally.

	--------------

	Chaos monkey testing in staging. The goal is to ensure no fallback to directs.

	--------------

	Review debug endpoints on the relay gateway

	If there are any (?), make sure they can be hidden, eg. if DEBUG

	--------------

	Almost certainly need to adjust relay manage to build route matrix only including active relays

	There are just too many relays defined, esp. all google and AWS ones...

	--------------

	Remember to bring back logic to exclude near relays with significantly higher than average jitter

	--------------

	Implement "next sessions"

	Implement "next servers"

	--------------

	Get raspberry backend, raspberry servers and raspberry clients running inside docker compose, so we can have a decent amount of activity to show by navigating to the portal URL locally.

	--------------

	Idea. "next status" gives a quick overview of the whole system, what services are up, what they are doing etc.

	--------------

	Try to simplify route shader parameters.

	RTT Veto doesn't really need multiple values.

	Latency Threshold would be much better named as "Minimum Latency Reduction" etc.

	Why have "reduce latency" and "reduce packet loss" bools at all? Of *course* you always want this... etc.

	--------------

	Lumen has VMs and bare metal via terraform!!!

	https://registry.terraform.io/providers/LumenTech/lumen/latest/docs/guides/getting_started

	-----------------------------------

	Relay name should get validated to have the datacenter name substring in it

	-----------------------------------

	XBox preferred port. Do we need to do anything?

	https://learn.microsoft.com/en-us/gaming/gdk/_content/gc/networking/overviews/game-mesh/preferred-local-udp-multiplayer-port-networking

	-----------------------------------

	Idea. Accumulated event flags, per-session summary.

	Allows to quickly look at sessions that experienced certain events vs. not, without needing to look at slices.

	-----------------------------------
