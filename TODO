DONE

	Pass over and check debug analytics
		
	networknext-analytics@sloclap.com

	timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)

	timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 48 HOUR)

TODO

	Amazon relays don't seem to be carrying sessions. Is this because of the price changes?

	When was the last time they carried sessions?

	The amazon relays are fully red for all suppliers except amazon. WTF?

	--------

	Fix GCORE MAC address issue

	*** STOP ***

































































NEXT MINOR SDK UPDATE:

	Client should know user_hash, match_id (post-upgrade)

	Make sure GetUserHash is implemented for client socket

	Extend Redpoint EOS module to have nice helper functions for setting game latency, jitter and PL (static)

	Is there anything we can add to help diagnose all client relays are zero?




















NEXT ITERATION:

	------------

	Safety todo in next.go for "next stop <pattern>"

	------------

	Fallback to direct is *not* caused by server <-> backend comms, because when acceleration was fully disabled, fallbacks stopped happening too.

	Fallback to direct is being caused by relays not carrying traffic properly, when we send traffic across them.

	Most likely when we send a session request packet across, we are not getting the session response back...

	------------

	Remove best_latency_reduction (it's wrong...)

	------------

	Add latency_save, jitter_save, packet_loss_save to the session summary.

	------------

	Add a server_not_accelerated flag in the session summary. Makes it easy to exclude these.

	------------

	Add "packet_loss_save" to the session summary and route state as a bool.

	A packet loss save happens when direct packet loss is > 10%, and next_rtt>0 and next packet loss is < 1% 

	Update packet loss saves graph to look for this bool in summary.

	------------

	Add lag spike flags to session summary.

		next_lag_spike
		direct_lag_spike

	------------

	Add jitter spike flags to session summary.

		next_jitter_spike
		direct_jitter_spike
		real_jitter_spike

	------------

	Add packet loss spike flags to session summary

		next_packet_loss_spike
		direct_packet_loss_spike
		real_packet_loss_spike

	------------

	It seems that duration_on_next is sometimes zero, even though a session has spent time on network next.

	Find this bug and fix it.

	Example session:

		-8003309578772432229

	ps. Maybe this was a desync on session state read? If it's super rare and no longer happens, just ignore...

	------------

	Vast majority of "Brasil Sessions Above 100ms Not Fixed" are because "all_client_relays_are_zero"

	This is the thing to fix now.

	no client relays now 100% tracks all client relays are zero

	so this is all the same problem...

	------------

	Look close look at data in a few weeks to see if recent changes have caused jitter to go up on next since ~Oct 12-14th

	------------

	Is it possible to widen the 90th percentile of jitter separation between next and direct with better selection of near relays? Maybe!

	Idea, have two modes. One for most of jitter is < 10ms, and another for most of jitter is above 10ms, and a third mode where almost all jitter is high.

	One is about minimization and not picking bad when there are good options.

	The other mode is about, picking from the better ones.

	The last mode is about, look we're in a jitter spike, so... whatever!

	-----

	Try to identify why abort is triggering.

	-----

	What is causing the remaining lost routes to happen?

	Dig in on some sessions to see what's causing it...

	It seems to have a strong correlation to datacenter_id

	Highest route lost %:

		latitude.saopaulo      ->  0.96%
		datapacket.istanbul    ->  0.66%
		i3d.dubai              ->  0.53%
		gcore.ashburn          ->  0.51%
		hivelocity.losangeles  ->  0.49%
		uk2group.dallas        ->  0.43%

		...

	Lowest route lost %:

		google.* are all at 0.0

		uk2group.frankfurt     ->  0.14%
		datapacket.frankfurt   ->  0.28%

		etc...

	What's causing this variation?

	-----
















	-----

	Would be nice if I could get the session counts over to the left a bit so I don't need to include scrollbar in screenshots...

	-----

	Add a functional test to verify that we see the correct lat/longs passed up from the SDK

	-----

	Add a functional test to verify that we see server delta time min/max/avg passed up from SDK

	-----

	Add a functional test to verify we get game rtt, jitter and pl

	-----

	Add a functional test to verify we see flags

	-----

	Add functional test to verify we see match id from sessions

	-----

	Add functional test to verify we get the likely_vpn_or_cross_region

	-----

	If we end up doing source relay filtering for jitter, add some unit tests and func tests around this.

	-----

	Update documentation

	-----

	Fix some easy issues

	-----

	Make release

	-----











	-----------

	Next release

	-----------

	There's a lot of stuff that sets "state.Error" kinda assuming its persistent, but it's not.

	I'd feel more comfortable migrating this over to bools instead of flags.

	And the bools must be persisted in route state and then set in the session summary.

	Can't make this change live with REMATCH because it will remove some visibility of things going back before this change.

	-----------

	Remove multipath. We'd never operate without it.

	Remove mispredict.

	Remove latency worse.

    -----------

    Replace fallback to direct with trying again.

	-----------

	Speed up the relay clean shutdown. There is no need to wait a full minute.

	Waiting until the backend acknowledges the relay is shutting down + 15 seconds should be sufficient.

	-----------

	Not being able to lookup a relay from the same relay_id (name) when it is resized/recreated with a new IP address is a real bummer.

	-----------
