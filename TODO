DONE

	Pass over and check debug analytics

	Downsized amazon relays

	Talk with Benoit about enabling acceleration in Asia Pacific

	Write a blog post containing data about results for REMATCH

	Debug sharing of bigquery tables between Network Next and Mas Bandwidth google cloud accounts

	Fix GCORE MAC address issue

TODO

	*** STOP ***

































































NEXT MINOR SDK UPDATE:

	Client should know user_hash, match_id (post-upgrade)

	Make sure GetUserHash is implemented for client socket

	Extend Redpoint EOS module to have nice helper functions for setting game latency, jitter and PL (static)

	Is there anything we can add to help diagnose all client relays are zero?




















NEXT ITERATION:

	------------

	Safety todo in next.go for "next stop <pattern>"

	------------

	Fallback to direct is *not* caused by server <-> backend comms, because when acceleration was fully disabled, fallbacks stopped happening too.

	Fallback to direct is being caused by relays not carrying traffic properly, when we send traffic across them.

	Most likely when we send a session request packet across, we are not getting the session response back...

	------------

	Remove best_latency_reduction (it's wrong...)

	------------

	Add latency_save, jitter_save, packet_loss_save to the session summary.

	------------

	Add a server_not_accelerated flag in the session summary. Makes it easy to exclude these.

	------------

	Add "packet_loss_save" to the session summary and route state as a bool.

	A packet loss save happens when direct packet loss is > 10%, and next_rtt>0 and next packet loss is < 1% 

	Update packet loss saves graph to look for this bool in summary.

	------------

	Add lag spike flags to session summary.

		next_lag_spike
		direct_lag_spike

	------------

	Add jitter spike flags to session summary.

		next_jitter_spike
		direct_jitter_spike
		real_jitter_spike

	------------

	Add packet loss spike flags to session summary

		next_packet_loss_spike
		direct_packet_loss_spike
		real_packet_loss_spike

	------------

	It seems that duration_on_next is sometimes zero, even though a session has spent time on network next.

	Find this bug and fix it.

	Example session:

		-8003309578772432229

	ps. Maybe this was a desync on session state read? If it's super rare and no longer happens, just ignore...

	------------

	Vast majority of "Brasil Sessions Above 100ms Not Fixed" are because "all_client_relays_are_zero"

	This is the thing to fix now.

	no client relays now 100% tracks all client relays are zero

	so this is all the same problem...

	------------

	Look close look at data in a few weeks to see if recent changes have caused jitter to go up on next since ~Oct 12-14th

	------------

	Is it possible to widen the 90th percentile of jitter separation between next and direct with better selection of near relays? Maybe!

	Idea, have two modes. One for most of jitter is < 10ms, and another for most of jitter is above 10ms, and a third mode where almost all jitter is high.

	One is about minimization and not picking bad when there are good options.

	The other mode is about, picking from the better ones.

	The last mode is about, look we're in a jitter spike, so... whatever!

	-----

	Try to identify why abort is triggering.

	-----

	What is causing the remaining lost routes to happen?

	Dig in on some sessions to see what's causing it...

	It seems to have a strong correlation to datacenter_id

	Highest route lost %:

		latitude.saopaulo      ->  0.96%
		datapacket.istanbul    ->  0.66%
		i3d.dubai              ->  0.53%
		gcore.ashburn          ->  0.51%
		hivelocity.losangeles  ->  0.49%
		uk2group.dallas        ->  0.43%

		...

	Lowest route lost %:

		google.* are all at 0.0

		uk2group.frankfurt     ->  0.14%
		datapacket.frankfurt   ->  0.28%

		etc...

	What's causing this variation?

	-----
















	-----

	Would be nice if I could get the session counts over to the left a bit so I don't need to include scrollbar in screenshots...

	-----

	Add a functional test to verify that we see the correct lat/longs passed up from the SDK

	-----

	Add a functional test to verify that we see server delta time min/max/avg passed up from SDK

	-----

	Add a functional test to verify we get game rtt, jitter and pl

	-----

	Add a functional test to verify we see flags

	-----

	Add functional test to verify we see match id from sessions

	-----

	Add functional test to verify we get the likely_vpn_or_cross_region

	-----

	If we end up doing source relay filtering for jitter, add some unit tests and func tests around this.

	-----

	Update documentation

	-----

	Fix some easy issues

	-----

	Make release

	-----











	-----------

	Next release

	-----------

	There's a lot of stuff that sets "state.Error" kinda assuming its persistent, but it's not.

	I'd feel more comfortable migrating this over to bools instead of flags.

	And the bools must be persisted in route state and then set in the session summary.

	Can't make this change live with REMATCH because it will remove some visibility of things going back before this change.

	-----------

	Remove multipath. We'd never operate without it.

	Remove mispredict.

	Remove latency worse.

    -----------

    Replace fallback to direct with trying again.

	-----------

	Speed up the relay clean shutdown. There is no need to wait a full minute.

	Waiting until the backend acknowledges the relay is shutting down + 15 seconds should be sufficient.

	-----------

	Not being able to lookup a relay from the same relay_id (name) when it is resized/recreated with a new IP address is a real bummer.

	-----------

	next_client_event

	next_server_event

	next_client_data

	next_server_data

	pass in initial data on upgrade per-client

	-----------

	XDP relay could have some config removed, and just have it compiled in to the xdp program, esp. the gateway address and anything we branch on

	-----------

	Can get gateway IP address like this:

		ip route show default | awk '/default/ {print $3}'

	And then can look into ARP cache, and look up for that IP address what MAC address it is.

		grep "$(ip route show default | awk '/default/ {print $3}')" /proc/net/arp | awk '{print $4}'

	A better way is to get the IP, and then check it vs "ip neigh"

		ip neigh

	Can also just look into the arp cache:

		arp -a

		_gateway (213.156.152.1) at 00:1c:73:00:20:ff [ether] on bond0
		? (213.156.152.107) at d4:f5:ef:a2:f1:4c [ether] on bond0

	We could periodically do this on the relay, to look up the default gateway IP address?

	We could pump the arp cache -a, match IP -> gateway, and default to the gateway if we don't find an entry.

	This could be passed up to the relay XDP program via a bpf map.

	I think this is the best solution, because it requires no manual configuration.

	-----------
