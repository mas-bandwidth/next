DONE

	Fix happy path.

TODO

	Make sure all func tests and load tests pass.

	------------------------------------------------------------

	What data should go in server update if any?

	I really can't think of any. I thnik we need to just implement the mapping from server to the sessions in it.

	Implement it via hash with HSET and minute buckets. Manually expire entries with last update time to get connected sessions within 30 seconds.

	Finish implementing "getServers" so it actually gets all server data, including the set of sessions that are currently connected to the server.

	---------------------------------------------------------------

	Insert relay update data (equivalent to slice data)

	What data should be in it? The only data that I can think of is a history of the num sessions, going back as long as possible really.

	Maybe also history of bandwidth etc... ? Yes.

	Should keep data per-relay for a maximum of 1 hour. 60*60 = 3600... that's OK.

	In reality, I'll probably only display data in the last 10 minutes, eg. 10*60 = 600 samples.

	Work out how to insert RPUSH, while removing any old entries past n=3600...

	Fully implement "GetRelays" so it gets both the relay data and the history of relay updates

	---------------------------------------------------------

	Bring across the new code (inserter) into the portal cruncher

	Make sure the portal cruncher doesn't block on any redis streams messages, it must flush periodically.

	---------------------------------------------------------

	We shoud shift slice number to the left one, and drop the first slice that doesn't know yet any RTTs

	This will make everything much more consistent, esp. portal and analytics that don't need to special case slice 0 RTT anymore.

	---------------------------------------------------------

	"Score" for session sorting on portal should use largest RTT reduction seen in the last n slices

	Make it smoother and more stable, also highight spike fixes.

	This can be calculated in-place in the cruncher, maybe? Do messages from the same source end up getting processed on the same dest, over time? I think they do.

	---------------------------------------------------------

	We probably need multiple "inserters" for pubsub and redis streams, switched on by session id, so we can effectively distribute processing. 

	Otherwise, with only one inserter and one server backend (typical) it will not distribute processing across multiple portal crunchers evenly.

	Distribute according to modulo session id, or hash of server address + port, or relay id

	---------------------------------------------------------












































































































































	--------------

	Get pubsub emulator setup.

	Now actually write the analytics messages to pubsub in happy path (local).

	Verify that the analytics service receives and processes the analytics messages.

	Setup the bigquery emulator.

	Actually write the analytics messages to bigquery...

	--------

	Emit the analytics database update message from relay_backend leader.

	--------

	Add server sdk to link to match

	next_server_match ( match_id )

	Make sure the match id gets passed up in both the server init, server update and the session updates from SDK -> backend.

	--------

	Add uint64 type to match data function

	Extend SDK to support multiple calls to match data

	Extend SDK to store an array of match data, and flush that array

	Extend match data packets to server backend to include type

	--------------

	Match events ()

	next_server_match_event

	--------------

	Session events (address)

	next_server_session_event

	--------------

	Session data (address, type, array of doubles)

	next_server_session_data

	--------------

	Get the pubsub emulator back up locally

	Get the process for setting up pubsub queues etc going via "run"

	eg. "run pubsub-create", "run pubsub-destroy" or whatever works

	--------------

	Get the bigquery emulator up locally

	Decide on schemas for each of the table types, and put them under "bigquery"

	eg. "run bigquery-create", "run bigquery-destroy" or whatever.

	--------------

	Extend analytics service to insert messages into bigquery.

	--------------

	Implement simple "api" service

	--------------









































Essentials:

	--------------

	The relay should talk to the relay backend over SSH in dev and production.

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Implement a fix for a re-ordering of route tokens / continue tokens to create loops. Order of tokens must be enforced.

	This probably means we need to have some sort of signed bit, that indicates that indeed, this is token n, n+1 etc.

	And then this can be checked.

	Needs to be designed...

	--------------

	The SDK must never transmit the user hash in plaintext. It must always be encrypted.

	--------------

	Get "pro" tagging back in. This is a cool feature.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Multipath across two network next routes (Psyonix).

	--------------





Nice to have:

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	---------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	--------------

	Extend the SDK5 so we have the option of doing near relay pings at multiple times throughout the session.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Disable SSH related actions in "local". They don't have anywhere to go...

	--------------

	Functional tests to verify that relay gateway, relay backend, server backend 5 are resilient to invalid or won't read in bad database.bin files

	--------------

	There should be relay functional tests.

	---------------

	I really can't answer the question "why are half the relays not carrying sessions in dev?"

	Is it just a property of the optimizer, eg. it will go away if I disable mispredict and set all costs to zero?

	Or is there some sort of bias causing this. It seems important to explore this and work it out.

	If there is some systemic bias that just makes some relays carry a lot of sessions while other relays don't... that's a serious problem in production.

	---------------

	Leader election func tests for relay backends + ready delay

	---------------

	Extend "next relays" to show "PublicAddress" and "InternalAddress" for relays.

	---------------

	Relay gateway verify keypair on startup

	Server backend verify keypairs on startup

	---------------

	Should be func tests that spin up the bigquery emulator and verify that each analytics message type can be written to the appropriate table created from schema

	---------------

	Add internal session events. Track server hitches, client hitches, no packets received, over bandwidth etc.

	These should be passed on to both the portal for immediate visualization, and to bigquery for analysis.

	Plumb internal events all the way up from client SDK to backend.

	Client SDK must be able to trigger certain events (eg. client hitches, no packets received etc...)

	Server can trigger others, then they get OR'd before sending up to backend.

	We need a way to track no packets on client for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	We need a way to track no packets on server from session for 0.1, 0.25, 0.5, 1.0 sec... from main thread.

	--------

	Redo pro, but do it via internal session events.

	--------
