DONE

	When the server backend is in the stopping or suspending state, set the lb_health check to false.

	curl http://metadata.google.internal/computeMetadata/v1/instance/id -H Metadata-Flavor:Google

	Verify the shutting down detection in dev

	Nope. It doesn't work, the stopping state comes to the VM too late.

	Moving to the managed instance group:

		gcloud compute instance-groups managed list-instances server-backend --region us-central1

	I believe the instance group will list the VM as stopping early, so I can key off this.

	Verify early stopping detection in dev. Nope.

	Try again... watch the MIG with console manually while it is stopping...

	Add manual connection drain post-SIGTERM.

	Increase rate of checking for DELETING status.

	Take code to staging and verify that no fallback to directs occur when scaling down server backend instances

	It seems mostly effective, but sometimes fallback to directs get through. sometimes no retries happen. sometimes retries happen, but no fallback to direct.

	Ideally, what I need to do is set a cooldown period or connection drain.

	Tried to set a shutdown script, but is it really having any effect?

	I think the heredoc syntax was off, trying again...

	Set a dependency for the raspberry client, server and backend on the server backend in dev.

	This way it won't start up until the new server backend is ready.

	Can we stop the server backend service manually with systemctl stop, and not have fallback to direct?

	If so, then it should be possible.

	Yes, it seemed to work.

	It seems the shutdown script is running when I have a regular VM

		sudo journalctl -fu google-shutdown-scripts.service

	However, when it runs it doesn't wait 60 seconds for the app service to stop...

	Leading me to believe that the app service is stopped more aggressively than SIGTERM (?!) when the system is stopped, and THEN the shutdown script is called.

TODO

	Seems like just putting the sleep in there makes it work.

	-----------------------




































	-----------------------

	Set server backend to have a dependency on the database, and redis instances in staging

	Set load tests to have a dependency on server backend, this way they don't start up until everything is ready.

	-----------------------



































































	---------------

	We need to see a steady route matrix across deploys. Work out what is going on with leader election and the new instance sending time series and counter data

	---------------



































































	--------------

	Fallback to direct is weird. It semes like I see repeated fallback to direct in server backends perpetually, almost like they are triggering fallback to direct over and over.

	--------------

	It seems that redis time series is quite underpowered in staging. With only 1M sessions, it has a very long delay (minutes) for counter aggregation.

	It needs more CPU. How to sort this out?

	--------------

	Add "portal_cruncher" counter. We need to see the throughput of the portal cruncher, to see if it is getting backed up or behind.

	Add "session_cruncher" counter. We need to see the input to the session cruncher.

	--------------

	There seems to be rapid leader flapping in relay backend on deploy. You can see this by looking at oscillations in route matrix size and total routes in admin view.

	Can I reproduce this in dev? If I can reproduce, then I can fix it in dev.

	(Perhaps a fix could be, don't consider yourself a leader until you have a > 0 routes in the route matrix?)

	--------------

	Number of routes is going up/down a lot even in dev. Is the minute history buffer long enough?

	--------------
	
















	--------------

	Could I be creating too many insertion connections into redis and making it not scale as well as it could? Tuning is probably required here.

	--------------

	Is there something I can do to ensure that all data for a session ends up on the same redis cluster?

	Slice data is what needs to be looked at very closely...

	--------------

	The server sessions not working @ 10M needs to be looked at closely

	It is most likely that too many sessions are locally created per-server, and thus, the overload occurred with the minute, minute - 1 stuff.

	But it is an average of only 100 sessions per-server, which is totally ordinary and to be expected with modern games....

	--------------

	Still getting some data holes in session slices @ 10M. Which part is getting overloaded? Portal cruncher? Redis?

	--------------

	There seems to be a load related issue @ 1000 relays.

	--------------

	Fix the fallback to direct when scaling down server backend instances.

	--------------

	Change analytics to perform bulk inserts instead of streaming inserts.

	https://cloud.google.com/bigquery/docs/write-api-batch

	--------------

	Add option to specify that only sessions with improvement > X get sent to the portal.

	This option would make it possible to significantly reduce the cost of running redis for the portal.

	If this option is set, session counts need to be grabbed from the redis counters, not the session cruncher (as it will be inaccurate)

	--------------













Small things:

	--------------

	If a single time series or counter key does not exist, all data for the "set" of pipelined ts.range will fail.

	This seems really bad. You'd expect that it would fail to execute only for the cmds that don't have keys.

	--------------

	The sort order for sessions appears incorrect according to score... not sure why.

	--------------

	Y axis labels spill over the right side when they get up to 100,000, 1,000,000 etc... convert to 500K, 1M, 2M?

	--------------

	Current sessions on server doesn't seem to be correct, or at least, it is 181 when it should be just 1...

	--------------

	uPlot graphs really need to calculate the real maximum from the data passed in

	In so many cases, they fail to calculate it. It's extremely annoying.

	--------------

	I need more space on a standard macbook air screen in the sessions list for longer ISP names

	Right now it is way too tight. A long ISP name would throw the whole layout off

	--------------

	There will be a challenge getting prod relays up in AWS vs. dev. There are no projects to segregate?

	Might need to create a separate project, or distinguish resources with naming convention.

	--------------

	Connection type detection needs to be brought back for all platforms in the SDK. On linux, connection type was 0 -> "Unknown"

	--------------

	Session counts on relays being 8 when relay backend restarts seems a bit suspicious. Are we not decrementing session counts somewhere?

	--------------

	Probably good to provide a way to disable the high priority threads on server with env var.

	When many server instances are running on one server, this can cause problems. eg. thread starvation

	--------------










Finalize terraform and document:

	Setup projects and service accounts with terraform

	--------------

	Relays need to be setup to use cloud storage for tf state

	--------------








Finalize SDK and UE5 plugin:

	------------------

	Update to latest PS4 and PS5 SDK on Windows PC

	Verify that we can build, link and run across PS4

	Verify that we can build, link and run across PS5

	------------------

	Update to latest XDK

	Verify that we can build, link and run across XBoxOne

	Verify that we can build, link and run across SeriesX

	------------------

	Setup PS4 compilation with custom agents

	Setup PS5 compilation with custom agents

	Setup XBoxOne compilation with custom agents

	Setup Series X compilation with custom agents

	------------------

	Get the UE5 plugin back up

	Make sure to include Flush on the server before the server is destroyed

	------------------
