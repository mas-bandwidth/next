
psql postgres
CREATE ROLE admin WITH LOGIN PASSWORD 'password';
ALTER ROLE admin CREATEDB;

psql postgres -U admin

DROP USER newuser;

drop database nn;

create database nn;























































	---------------

	You don't seem to be able to edit the relay public key in the admin tool?

	Ideally, we should be able to edit the relay address, the relay public key to adjust a relay if we need to roll a new key.

	Pretty confident right now that certain relays in dev are misconfigured, with the wrong public key set in relay.env vs. in the database for this reason.

	---------------

	We need a way to check relay public keys set in relay.env vs. relay public keys set in the database / database.bin

	Normally, this should happen when the relay fails to init because the signature check fails.

	But right now it's failing to happen because the relay 

	---------------

	Relays should not be able to go "online" unless the relays are enabled.

	eg. google.southcarolina was totally disabled/removed/deleted -- but still showed up in online relays. wtf?

	And now somehow google.southcarolina is in the database.bin in dev, even though it's deleted? I don't know how...

	---------------

	Raspberry pi clients can ping relays now, but they quickly fail because the route request doesn't get through.

	What's going wrong?

	-----------------------------

Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: ------------------------------
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: 100010.550125: debug: client received route update packet from server
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: --------------------------------------
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: 21/29 source relays are routable
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: 1 dest relay
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: try to reduce latency
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: forcing network next
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: best route cost is 36
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: found 24 suitable routes in [36,136] from 21/29 near relays
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: route diversity 17
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: take network next: google.virginia.3 - google.iowa.1
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: route cost is 53
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: multipath
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: --------------------------------------
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: 100010.550165: info: client near relay pings completed
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: 100010.550668: info: client next route
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: 100010.550690: info: client multipath enabled
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: 100010.550729: debug: client sent route update ack packet to server
Jan 23 02:44:42 raspberry-client-mig-jwz0 app[91155]: 100010.550779: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:43 raspberry-client-mig-jwz0 app[91155]: 100010.897979: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:43 raspberry-client-mig-jwz0 app[91155]: 100011.234007: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:43 raspberry-client-mig-jwz0 app[91155]: 100011.545971: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:44 raspberry-client-mig-jwz0 app[91155]: 100011.857868: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:44 raspberry-client-mig-jwz0 app[91155]: 100012.137913: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:44 raspberry-client-mig-jwz0 app[91155]: 100012.449866: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:45 raspberry-client-mig-jwz0 app[91155]: 100012.761865: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:45 raspberry-client-mig-jwz0 app[91155]: 100013.039031: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:45 raspberry-client-mig-jwz0 app[91155]: 100013.349806: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:46 raspberry-client-mig-jwz0 app[91155]: 100013.661822: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:46 raspberry-client-mig-jwz0 app[91155]: 100013.973834: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:46 raspberry-client-mig-jwz0 app[91155]: 100014.233785: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:46 raspberry-client-mig-jwz0 app[91155]: 100014.545766: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:47 raspberry-client-mig-jwz0 app[91155]: 100014.857713: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:47 raspberry-client-mig-jwz0 app[91155]: 100015.133729: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:47 raspberry-client-mig-jwz0 app[91155]: 100015.445634: debug: client sent route request to relay: 35.236.236.4:40000
Jan 23 02:44:47 raspberry-client-mig-jwz0 app[91155]: 100015.653652: error: client route request timed out
Jan 23 02:44:47 raspberry-client-mig-jwz0 app[91155]: 100015.653691: info: client fallback to direct


































	-------

	Verify that we have raspberry pi's running in dev.

	Now scale up the number of clients and servers to at least 100 clients.

	-------

	If raspberry client falls back to direct, restart the client ASAP to save time.

	-------

	Reference relay needs the upgrade functionality implemented

	This way we can easily switch between reference and real relay.

	Right now, the reference relay is a dead end.

	-------

	The relay needs to get and store the upcoming, current and previous magic values

	These values need to be accessible by the relay threads

	How to best do this?

	-------

	Check the advanced packet filter on the relay ping packet

	Verify the near relay ping packets still get through

	-------

	Write the response pong packet

	Make sure it passes the basic and advanced packet filters

	Verify that the client5 actually gets the pongs

	-------

	Deploy new relay version in dev

	Verify that raspberry pi's now get near relay pings and go across network next

	-------
















Essentials:

    --------------

    Send portal messages to redis streams

	--------------

	Portal cruncher new

	--------------

	Send rest of server backend messages to google pubsub

	--------------

	Extend analytics service to insert messages into bigquery.

	--------------

	Leader election needs to wait at least 11 seconds to make sure it gets the correct result when > 2 vms start at the same time without flap

	--------------

	Leader election func testing for relay backends + ready delay

	--------------

	Implement func test for server backend to make sure when it is in connection drain state, it indicates to the LB health test that traffic should not be sent to it.

	--------------

	Extend the relay to support a secondary relay backend.

	This will enable relays to be shared between dev and dev5, and prod and prod5.

	--------------









Nice to have:

	-----------

	Rename to "game events" all the way down to the sdk5

	-----------

	Add match id to both portal data and session update data by default (first slice only, and summary...)

	Then work out a new API to link the session with a match (start of session, with tags...), separate from setting match data (end of match).

	-----------

	Add unit test to make sure we write out session update message

	Add unit test to make sure we write out portal message

	Add unit test to make sure we write out near relay message 

	--------------

	Simplified debug string for first slice: "first slice always goes direct" without a bunch of other junk.

	Printing out names of near relays would be nice etc.

	--------------

	Code in service.go to only shut down service once various subsystems have reported that they're complete

	Extend this to the server backend to make sure that we flush all channels of messages before shutting down

	--------------

	Change SDK to pass up the real packet loss, real jitter, real out of order etc.

	We don't need to store multiple uint64 in session data to calculate this

	It's wasteful to pass this data back to the client with each session update response.

	--------------

	Create a new message for near relay stats.

	Unit test it, then on slice #1 when near relay data comes in, write a near relay ping stats message and send it.

	--------------

	Implement the near relay ping token.

	We need to do this now, so we don't have problems with it in the future.

	It can be as simple as an expire time for pings

	The relay can also have a count of the maximum number of pings to reply to, eg. 10 * 10 = 100.

	--------------

	Implement a fix for a re-ordering of route tokens / continue tokens to create loops. Order of tokens must be enforced.

	This probably means we need to have some sort of signed bit, that indicates that indeed, this is token n, n+1 etc.

	And then this can be checked.

	Needs to be designed...

	--------------

	Extend the SDK5 so we have the option of sending down new near relay stats on multiple slices, later on.

	Use a near relay ping sequence # (uint8) so we can tell when we have new near relay pings that we should upload to the backend.

	Add a func test to make sure we capture this functionality. We want the option to redo near relay pings on later slice, in the future without changing the SDK.

	--------------

	Multipath across two network next routes.

	--------------

	It would be good to track when sessions shut down, vs. having them always time out

	This would require new messages up then back down the chain of relays for ack

	It would also reduce the risk of missing packets or having sessions time out and not noticing.

	We can now track sessions timing out before being closed, and this would indicate problems (or at least, hard disconnects...)

	Again, adding this later would be challenging. Best to do it now.

	--------------

	Rework the reference relay and make it the official relay.

	The trick is to use the code from the proxy to go wide across n threads with SO_REUSEPORT

	Since each port/address maps to a specific socket, there is no need for locks across the session map, there will be a session map per-thread.

	Stats can be managed with atomics, eg. count of sessions per-thread in a uint64 atomic, summed by the main thread before uploading the total to the relay backend.

	--------------

	A relay which has its firewall misconfigured will cause players to fallback to direct, because traffic can't be sent to it.

	Would be nice if the relay backend would talk to the relay over its UDP port and say hi, and if this heartbeat was a prerequisite for the relay coming online.

	This way misconfigured relays can't cause problems.

	--------------

	The SDK must never transmit the user hash in plaintext. It must always be encrypted.

	--------------

	Consider renaming "Server Events" to "Game Events" in SDK5

	--------------

	Put "MatchId" into session data (first).

	This way we can easily look up from a session the match it is in.

	--------------

	I'm not sure the expire timestamp in the server backend was even working correctly.

	Investigate how the expire timestamp was used, calculated and if it was even doing anything...

	Also investigate relay concept of time in reference vs. real relay

	--------------
