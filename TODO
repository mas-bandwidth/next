DONE

	Create a channel specifically for fallback for fallback to direct

	Ingest into bigquery with google pubsub

	"Better yet, you no longer need to pay for data ingestion into BigQuery when using this new direct method. You only pay for the Pub/Sub you use. Ingestion from Pub/Subâ€™s BigQuery subscription into BigQuery costs $50/TiB based on read (subscribe throughput) from the subscription. This is a simpler and cheaper billing experience compared to the alternative path via Dataflow pipeline where you would be paying for the Pub/Sub read, Dataflow job, and BigQuery data ingestion. See the pricing page for details."

	https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics

	Investigate apache avro vs. protobufs. Both options are available for google pubsub.

	Going with avro.

	Define avro message schemas, copying from bigquery json and translating.

	Implement golang side, structs with avro tags and encoding methods.

	Implement a basic test to verify that each analytics struct writes with avro schema.

	Setup pubsub channels to have bigquery table and schema defined for messages.

	Debug and fix a huge amount of issues...

	Convert the timestamp fields to int64 with logical types, see:

		https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions

	Make sure all timestamps are passed in with micro precision.

	Actually send the messages again from the session update handler.

	Load the avro schemas in the server backend.

	Process the session update messages in the server backend and encode them with avro before sending to google pubsub.

	Deploy to dev and verify data is ingested into bigquery.

	No, it's not ingested. I think the AVRO schema is set to text (JSON) messages. I need to set the encoding to binary.

	Set to binary and deploy.

	Verify bigquery sees data.

	Force next flag is true in dev on session slices. Is this correct? Yes.

TODO	

	-------------

	Pass over the rest of the server backend messages, make sure they are emitted with correct timestamps (micros), and extend the handlers to encode to avro and send to google pubsub

	-------------

	Pass over the relay backend messages. Make sure timestamp is micros, and extend handlers to encode in avro and send to google pubsub.

	-------------

	Update staging so the pubsub service account has permission to do the bigquery stuff.

	-------------

	Deploy to staging and verify we have ingestion into bigquery @ 100k sessions.

	-------------

	Scale up to 1000 relays, 1M servers and 10M sessions and run for one day.

	Verify that we have data ingestion running for the whole day with no errors.

	-------------

	At this point the backend is finished.

	-------------























































	----------

	Integrate map with portal

	----------

	Setup projects and service accounts with terraform

	----------

	Write documentation

	----------


























Small things:

	----------

	How to make sure bigquery partitions on my timestamp, not the insert timestamp?

	----------

	"Active relays" seems to be cosmetically messed up during load test, it shows only 100 active relays in the chart, but there are 1000 active relays for sure.

	----------

	Calling sessions "next" is fine for code, but in the portal we should refer to it as accelerated.

	That way if somebody rebrands the portal internally, stuff will still make sense vs. "next sessions" (what's that?)

	----------

	Make sure the system can run with no relays in dev and production (eg. analysis only initially...)

	----------

	Relays need to be setup to use cloud storage for tf state

	----------

	For providers like akamai, vultr, amazon, we need to prefix all resource names with env (because their aren't separate projects...)

	--------------

	Clean up old ip2location database files in server backend

	--------------

	User hash link in server sessions page is not working

	--------------

	Just leave the buyer page as graphs. The rightmost column doesn't add enough to justify its existence.

	--------------

	Click on server to see sessions on server is broken in staging even at small load. My guess, sessions are not properly linking to server addresses?

	--------------

	Active relays seems to double during deploys. Currently, I cannot think of any reason why this would happen?

	--------------

	Extend session update so it sends all sessions to portal below 100k sessions total, even if portal next sessions only is set

	--------------

	Relay graphs need to be aggregated (time series) into per-minute values. Use average.

	--------------

	Y axis labels spill over the right side when they get up to 100,000, 1,000,000 etc... convert to 500K, 1M, 2M?

	--------------

	Current sessions on server doesn't seem to be correct, or at least, it is 181 when it should be just 1...

	--------------

	uPlot graphs really need to calculate the real maximum from the data passed in

	In so many cases, they fail to calculate it. It's extremely annoying.

	--------------

	I need more space on a standard macbook air screen in the sessions list for longer ISP names

	Right now it is way too tight. A long ISP name would throw the whole layout off

	--------------

	There will be a challenge getting prod relays up in AWS vs. dev. There are no projects to segregate?

	Might need to create a separate project, or distinguish resources with naming convention.

	--------------

	Connection type detection needs to be brought back for all platforms in the SDK. On linux, connection type was 0 -> "Unknown"

	--------------

	Session counts on relays being 8 when relay backend restarts seems a bit suspicious. Are we not decrementing session counts somewhere?

	--------------

	Probably good to provide a way to disable the high priority threads on server with env var.

	When many server instances are running on one server, this can cause problems. eg. thread starvation

	--------------






Finalize SDK and UE5 plugin:

	------------------

	Update to latest PS4 and PS5 SDK on Windows PC

	Verify that we can build, link and run across PS4

	Verify that we can build, link and run across PS5

	------------------

	Update to latest XDK

	Verify that we can build, link and run across XBoxOne

	Verify that we can build, link and run across SeriesX

	------------------

	Setup PS4 compilation with custom agents

	Setup PS5 compilation with custom agents

	Setup XBoxOne compilation with custom agents

	Setup Series X compilation with custom agents

	------------------

	Get the UE5 plugin back up

	Make sure to include Flush on the server before the server is destroyed

	------------------
