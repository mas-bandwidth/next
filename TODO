DONE

	Also watch "Latency Saves (% Daily)" we will see increases here as we fix more players.

	These is a signficant increase as I bumped packet loss sensitivity up to 0.1%. 

	I think this is because small amounts of packet loss tend to indicate a connection that might need help.

	Let SLOCLAP know about short sessions.

	Work up additional queries around short sessions

	Seems to be 8-9%, fairly uniform across regions, datacenters.

	There has been a significant reduction in vetos

	Look at "Debug Latency Not Fixed (% Daily)" to see if there is improvement.

	Significant reduction over the past few days

	Watch "Lost Route (% Daily)" to see if I can spot some improvement post fixes.

	Yes, there is a significant reduction.

	Write query to calculate max sessions per-relay

	Create a tool to print out relay ids

	Downsize google cloud.

	Google cloud billing looks like it is on a good trend, despite increased accel %. This is good!

	Fallback to direct % is really high overnight. This is super bad. What's going on?!

	What did I change in the evening that could be causing this?!

	Is it the server backend? Increased to 6 instances, don't see any improvement...

	Does the new hotfix include stuff related to network next?

	Hotfix was deployed after this started happening, so it's not related at all.

	But maybe it has some more data, like why the fallback to directs are happening.

	Is it the google relays?

	Stopping all google relays to see if they are contributing to the fallback to direct

	Wait a while and see if this improves it...

	Did not fix it. It's not the google cloud relays?

	Stopping all relays. If it's caused by a relay, then it will cease. If it's not caused by relays, but is related to the backend then it will keep happening (even though no acceleration is happening...)

	Fallback to direct has ceased. This means it is being caused by DDoS protection tripping on some relays. The pings between relays say the route is good, but when traffic goes through it, it is not being allowed through.

	Now I need to work out which relay or relays are causing this...

	Turning on datapacket...

	I'm getting some small amount of fallback to direct, but not a massive amount.

	Turning on google... it's fine

	Turning on amazon... it's fine

	Turning on zenlayer... it's fine

	Turning on akamai... seems fine

	Turning on i3d... seems fine

	Turning on latitude... seems fine

	Starting hivelocity and velia... seems fine, although I did see some fallback to direct spikes as new relays came on...

	Starting ovh and servers.com... seems fine

	Starting uk2group... seems fine

	Starting gcore... seems fine

	*sigh*

	No SDK update is seen, so there is no now data to look at regarding fallback to direct (yet)

	Finish reviewing analytics today and then stop. There is nothing urgent that needs to be done right now.

	I don't really trust the latency saves metric. It seems too good. It can't be 20% of sessions daily, re-implement it looking at per-slice data and double check it

	Pushed a fix. It was being calculated wrong in the session update handler.

	Verify fix for the latency saves metric.

	Nope. Still high. Implement another method that finds the maximum latency reduction per-session and then uses that.

	Confirmed, it's wrong. I'll need to create a new logic in session_update to track latency saves, the score based method seems flawed.

	Clean up and simplify SLOCLAP facing queries in bigquery ahead of sharing

	Finish converting across remaining queries to use rematch_* tables

	Work out how to share the SQL queries with sloclap (repository?)

	Share the queries

TODO


	*** STOP ***




































NEXT MINOR SDK UPDATE:

	Client should know user_hash, match_id (post-upgrade)

	Make sure GetUserHash is implemented for client socket

	Extend Redpoint EOS module to have nice helper functions for setting game latency, jitter and PL (static)

	Is there anything we can add to help diagnose all client relays are zero?




















NEXT ITERATION:

	------------

	Safety todo in next.go for "next stop <pattern>"

	------------

	Fallback to direct is *not* caused by server <-> backend comms, because when acceleration was fully disabled, fallbacks stopped happening too.

	Fallback to direct is being caused by relays not carrying traffic properly, when we send traffic across them.

	Most likely when we send a session request packet across, we are not getting the session response back...

	------------

	Remove best_latency_reduction (it's wrong...)

	------------

	Add latency_save, jitter_save, packet_loss_save to the session summary.

	------------

	Add a server_not_accelerated flag in the session summary. Makes it easy to exclude these.

	------------

	Add "packet_loss_save" to the session summary and route state as a bool.

	A packet loss save happens when direct packet loss is > 10%, and next_rtt>0 and next packet loss is < 1% 

	Update packet loss saves graph to look for this bool in summary.

	------------

	Add lag spike flags to session summary.

		next_lag_spike
		direct_lag_spike

	------------

	Add jitter spike flags to session summary.

		next_jitter_spike
		direct_jitter_spike
		real_jitter_spike

	------------

	Add packet loss spike flags to session summary

		next_packet_loss_spike
		direct_packet_loss_spike
		real_packet_loss_spike

	------------

	It seems that duration_on_next is sometimes zero, even though a session has spent time on network next.

	Find this bug and fix it.

	Example session:

		-8003309578772432229

	ps. Maybe this was a desync on session state read? If it's super rare and no longer happens, just ignore...

	------------

	Vast majority of "Brasil Sessions Above 100ms Not Fixed" are because "all_client_relays_are_zero"

	This is the thing to fix now.

	no client relays now 100% tracks all client relays are zero

	so this is all the same problem...

	------------

	Look close look at data in a few weeks to see if recent changes have caused jitter to go up on next since ~Oct 12-14th

	------------

	Is it possible to widen the 90th percentile of jitter separation between next and direct with better selection of near relays? Maybe!

	Idea, have two modes. One for most of jitter is < 10ms, and another for most of jitter is above 10ms, and a third mode where almost all jitter is high.

	One is about minimization and not picking bad when there are good options.

	The other mode is about, picking from the better ones.

	The last mode is about, look we're in a jitter spike, so... whatever!

	-----

	Try to identify why abort is triggering.

	-----

	What is causing the remaining lost routes to happen?

	Dig in on some sessions to see what's causing it...

	It seems to have a strong correlation to datacenter_id

	Highest route lost %:

		latitude.saopaulo      ->  0.96%
		datapacket.istanbul    ->  0.66%
		i3d.dubai              ->  0.53%
		gcore.ashburn          ->  0.51%
		hivelocity.losangeles  ->  0.49%
		uk2group.dallas        ->  0.43%

		...

	Lowest route lost %:

		google.* are all at 0.0

		uk2group.frankfurt     ->  0.14%
		datapacket.frankfurt   ->  0.28%

		etc...

	What's causing this variation?

	-----
















	-----

	Would be nice if I could get the session counts over to the left a bit so I don't need to include scrollbar in screenshots...

	-----

	Add a functional test to verify that we see the correct lat/longs passed up from the SDK

	-----

	Add a functional test to verify that we see server delta time min/max/avg passed up from SDK

	-----

	Add a functional test to verify we get game rtt, jitter and pl

	-----

	Add a functional test to verify we see flags

	-----

	Add functional test to verify we see match id from sessions

	-----

	Add functional test to verify we get the likely_vpn_or_cross_region

	-----

	If we end up doing source relay filtering for jitter, add some unit tests and func tests around this.

	-----

	Update documentation

	-----

	Fix some easy issues

	-----

	Make release

	-----





	-----------

	Next release

	-----------

	There's a lot of stuff that sets "state.Error" kinda assuming its persistent, but it's not.

	I'd feel more comfortable migrating this over to bools instead of flags.

	And the bools must be persisted in route state and then set in the session summary.

	Can't make this change live with REMATCH because it will remove some visibility of things going back before this change.

	-----------

	Remove multipath. We'd never operate without it.

	Remove mispredict.

	Remove latency worse.

    -----------

    Replace fallback to direct with trying again.

	-----------
