DONE

	Modify staging so only 10% of sessions take network next

	Add a flag to only send next sessions to portal

	Make API get next and total session counts from counters instead of session cruncher

	Extend portal cruncher to also send next_session_update counter

	Increase CPU power and memory for time series redis stack

	Set server backend to have a dependency on the database, and redis instances in staging

	Set load tests to have a dependency on server backend, this way they don't start up until everything is ready.

	Simplify session cruncher so it no longer does counts

	Simplify server crouncher so it no longer does counts

	Simplify session cruncher so it no longer tracks next sessions

	Verify in dev that we have next and total session counts

	Fix the session sort issue in highest score bucket

	Remove the graphs for "Server Updates" and "Session Updates

	Update admin graphs to use the new 

	Test that we have 10% of sessions on next in staging

TODO

	Scale up to 1M in staging with 10% next

	Scale up to 10M in staging with 10% next

	Verify that portal crunchers and redis have much smaller load

	Verify that we don't see any redis time series stretched dots @ 10M























































	

	-----------------------

	Simplify the top sessions and top server outputs

	Make sure load test portal works

	Make sure func test portal works

	-----------------------

	Implement counters for buyer servers and buyer sessions

	Update API to get all buyer related server and session counts from counters

	Remove cruft in api related to buyer session and server counts

	Bring back the accelerated percent graph

	-----------------------

	Make sure all admin graphs work in dev

	Make sure all buyer graphs work in dev

	-----------------------

	Extend session update so it sends all sessions to portal below 100k sessions total, even if portal next sessions only is set

	-----------------------



































































	---------------

	We need to see a steady route matrix across deploys. Work out what is going on with leader election and the new instance sending time series and counter data

	---------------



































































	--------------

	Fallback to direct is weird. It semes like I see repeated fallback to direct in server backends perpetually, almost like they are triggering fallback to direct over and over.

	--------------

	Add "portal_cruncher" counter. We need to see the throughput of the portal cruncher, to see if it is getting backed up or behind.

	Add "session_cruncher" counter. We need to see the input to the session cruncher.

	--------------

	There seems to be rapid leader flapping in relay backend on deploy. You can see this by looking at oscillations in route matrix size and total routes in admin view.

	Can I reproduce this in dev? If I can reproduce, then I can fix it in dev.

	(Perhaps a fix could be, don't consider yourself a leader until you have a > 0 routes in the route matrix?)

	--------------

	Number of routes is going up/down a lot even in dev. Is the minute history buffer long enough?

	--------------
	
















	--------------

	There seems to be a load related issue @ 1000 relays.

	--------------

	Change analytics to perform bulk inserts instead of streaming inserts.

	https://cloud.google.com/bigquery/docs/write-api-batch

	--------------













Small things:

	--------------

	If a single time series or counter key does not exist, all data for the "set" of pipelined ts.range will fail.

	This seems really bad. You'd expect that it would fail to execute only for the cmds that don't have keys.

	--------------

	Y axis labels spill over the right side when they get up to 100,000, 1,000,000 etc... convert to 500K, 1M, 2M?

	--------------

	Current sessions on server doesn't seem to be correct, or at least, it is 181 when it should be just 1...

	--------------

	uPlot graphs really need to calculate the real maximum from the data passed in

	In so many cases, they fail to calculate it. It's extremely annoying.

	--------------

	I need more space on a standard macbook air screen in the sessions list for longer ISP names

	Right now it is way too tight. A long ISP name would throw the whole layout off

	--------------

	There will be a challenge getting prod relays up in AWS vs. dev. There are no projects to segregate?

	Might need to create a separate project, or distinguish resources with naming convention.

	--------------

	Connection type detection needs to be brought back for all platforms in the SDK. On linux, connection type was 0 -> "Unknown"

	--------------

	Session counts on relays being 8 when relay backend restarts seems a bit suspicious. Are we not decrementing session counts somewhere?

	--------------

	Probably good to provide a way to disable the high priority threads on server with env var.

	When many server instances are running on one server, this can cause problems. eg. thread starvation

	--------------










Finalize terraform and document:

	Setup projects and service accounts with terraform

	--------------

	Relays need to be setup to use cloud storage for tf state

	--------------








Finalize SDK and UE5 plugin:

	------------------

	Update to latest PS4 and PS5 SDK on Windows PC

	Verify that we can build, link and run across PS4

	Verify that we can build, link and run across PS5

	------------------

	Update to latest XDK

	Verify that we can build, link and run across XBoxOne

	Verify that we can build, link and run across SeriesX

	------------------

	Setup PS4 compilation with custom agents

	Setup PS5 compilation with custom agents

	Setup XBoxOne compilation with custom agents

	Setup Series X compilation with custom agents

	------------------

	Get the UE5 plugin back up

	Make sure to include Flush on the server before the server is destroyed

	------------------
